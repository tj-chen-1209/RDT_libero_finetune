import os
import csv
import random

import numpy as np
import sys
import torch
import yaml
import argparse
from collections import deque
from PIL import Image

LIBERO_REPO_ROOT = "/home/zhukefei/chensiqi/LIBERO"

if LIBERO_REPO_ROOT not in sys.path:
    sys.path.insert(0, LIBERO_REPO_ROOT)

from libero.libero import get_libero_path
from libero.libero.benchmark import get_benchmark_dict
from libero.libero.envs import OffScreenRenderEnv
from libero_rdt_model import create_model, RoboticDiffusionTransformerModel
from libero.libero.utils.video_utils import VideoWriter 

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--task-id", type=int, default=0, help="Task ID in libero_10 (0-9)")
    parser.add_argument("--num-traj", type=int, default=25, help="Number of trajectories to test")
    parser.add_argument("--pretrained-path", type=str, required=True, help="Path to pretrained model")
    parser.add_argument("--dataset-name", type=str, default="libero_10", 
                        choices=["libero_10", "libero_90"], help="Dataset name")
    # æ·»åŠ è§†é¢‘å‚æ•°
    parser.add_argument("--save-videos", action="store_true", help="Save evaluation videos")
    parser.add_argument("--video-dir", type=str, default="outs/videos", help="Directory to save videos")
    # æ·»åŠ LoRAå‚æ•°
    parser.add_argument("--lora-weights", type=str, default=None, 
                        help="Path to LoRA weights (if using LoRA fine-tuned model)")
    parser.add_argument(
    "--metrics-path",
    type=str,
    default=None,
    help="è¯„ä¼°ç»“æœä¿å­˜çš„ CSV è·¯å¾„ï¼›å¦‚æœæä¾›ï¼Œåˆ™æ¯ä¸ª task è¿½åŠ ä¸€è¡Œ"
    )
    args = parser.parse_args()
     # ====== ä»»åŠ¡èŒƒå›´çº¦æŸï¼šæ ¹æ® dataset_name æ£€æŸ¥ task-id ======
    if args.dataset_name == "libero_10":
        if not (0 <= args.task_id < 10):
            parser.error("For dataset 'libero_10', --task-id must be in [0, 9].")
    elif args.dataset_name == "libero_90":
        if not (0 <= args.task_id < 90):
            parser.error("For dataset 'libero_90', --task-id must be in [0, 89].")

    return args

def set_global_seeds(seed: int):
    """
    ç»Ÿä¸€æ§åˆ¶ Python / NumPy / PyTorch çš„éšæœºç§å­ï¼Œä¾¿äºå¤ç°ã€‚
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def get_task_name_from_bddl(bddl_file_path):
    """ä» BDDL æ–‡ä»¶è·¯å¾„æå–ä»»åŠ¡åç§°"""
    filename = os.path.basename(bddl_file_path)
    return filename.replace('.bddl', '')


def extract_instruction_from_task_name(task_name):
    """ä»ä»»åŠ¡åç§°æå–æŒ‡ä»¤"""
    if task_name[0].isupper():
        scene_pos = task_name.find("SCENE")
        if scene_pos != -1:
            if "SCENE10" in task_name:
                language_part = task_name[scene_pos + 8:]
            else:
                language_part = task_name[scene_pos + 7:]
            return language_part.replace('_', ' ')
    return task_name.replace('_', ' ')


def load_language_embedding(task_name, dataset_name="libero_10", policy=None):
    """
    åŠ è½½è¯­è¨€åµŒå…¥ï¼ˆä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—ï¼Œå¦åˆ™åŠ¨æ€ç¼–ç ï¼‰
    """
    lang_embed_path = os.path.join("outs/libero_embeddings", dataset_name, f"{task_name}.pt")
    
    if os.path.exists(lang_embed_path):
        print(f"âœ“ Loading pre-computed embedding: {task_name}")
        lang_data = torch.load(lang_embed_path)
        # æå– embeddings é”®
        if isinstance(lang_data, dict):
            embeddings = lang_data['embeddings']
        else:
            embeddings = lang_data
        
        # ç¡®ä¿æœ‰ batch ç»´åº¦ [B, seq_len, hidden_dim]
        if embeddings.dim() == 2:
            embeddings = embeddings.unsqueeze(0)  # [seq_len, hidden_dim] -> [1, seq_len, hidden_dim]
        
        return embeddings
    else:
        if policy is None:
            raise ValueError(f"Language embedding not found and no policy provided")
        
        print(f"âš  Embedding not found, encoding on-the-fly: {task_name}")
        instruction = extract_instruction_from_task_name(task_name)
        return policy.encode_instruction(instruction)


def main():
    args = parse_args()
    
    # 1. åŠ è½½æ¨¡å‹ä¸é…ç½®
    print("Loading model...")
    config_path = 'configs/base.yaml'
    with open(config_path, "r") as fp:
        config = yaml.safe_load(fp)
    
    # ç»Ÿä¸€ device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[info] Using device: {device}")
    
    # åŸºç¡€éšæœºç§å­ï¼ˆæ¯ä¸ª episode ä¼šåœ¨æ­¤åŸºç¡€ä¸ŠåŠ åç§»ï¼‰
    base_seed = 20241201
    set_global_seeds(base_seed)
    
    pretrained_text_encoder_name_or_path = "google/t5-v1_1-xxl"
    pretrained_vision_encoder_name_or_path = "google/siglip-so400m-patch14-384"
    
    policy = create_model(
        args=config, 
        dtype=torch.bfloat16,
        pretrained=args.pretrained_path,
        pretrained_text_encoder_name_or_path=pretrained_text_encoder_name_or_path,
        pretrained_vision_encoder_name_or_path=pretrained_vision_encoder_name_or_path,
    )
    # Note: policy.reset() is already called in __init__ which handles device placement and eval mode
    
    # 2. è·å–ä»»åŠ¡ä¿¡æ¯
    benchmark_dict = get_benchmark_dict()
    task_suite = benchmark_dict[args.dataset_name]()
    task = task_suite.get_task(args.task_id)
    
    bddl_file = os.path.join(
        get_libero_path("bddl_files"),
        task.problem_folder,
        task.bddl_file,
    )
    
    task_name = get_task_name_from_bddl(bddl_file)
    instruction = extract_instruction_from_task_name(task_name)
    
    print(f"Task ID: {args.task_id}")
    print(f"Task Name: {task_name}")
    print(f"Instruction: {instruction}")
    
    # 3. åŠ è½½è¯­è¨€åµŒå…¥ï¼Œå¹¶æ¬åˆ° device / dtype
    text_embed = load_language_embedding(task_name, args.dataset_name, policy)
    text_embed = text_embed.to(device=device, dtype=torch.bfloat16)
    
    # 4. åˆ›å»ºç¯å¢ƒ
    env = OffScreenRenderEnv(
        bddl_file_name=bddl_file,
        camera_heights=128,
        camera_widths=128
    )
    
    # 5. è·å–åˆå§‹çŠ¶æ€
    init_states = task_suite.get_task_init_states(args.task_id)
    
    # 6. è¯„ä¼°å¾ªç¯
    MAX_EPISODE_STEPS = 400
    total_episodes = args.num_traj
    success_count = 0   # libero é£æ ¼ï¼šæœ‰ done=True çš„ episode æ•°
    
    import tqdm
    from collections import deque
    
    # åˆ›å»ºè§†é¢‘ä¿å­˜ç›®å½•
    video_folder = os.path.join(
        args.video_dir,
        f"{args.dataset_name}_task{args.task_id}"
    )
    
    # ä½¿ç”¨ VideoWriter ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    with VideoWriter(video_folder, save_video=args.save_videos, fps=30, single_video=False) as video_writer:
        for episode in tqdm.trange(total_episodes):
            # ä½¿ç”¨ä¸åŒçš„åˆå§‹çŠ¶æ€
            init_state_id = episode % len(init_states)
            
            # ä¸ºæ¯ä¸ª episode è®¾ç½®ç‹¬ç«‹ä½†å¯å¤ç°çš„ seed
            episode_seed = base_seed + episode
            env.seed(episode_seed)
            set_global_seeds(episode_seed)
            
            obs = env.reset()
            env.set_init_state(init_states[init_state_id])
            
            policy.reset()
            video_writer.reset()  # é‡ç½®è§†é¢‘ç¼“å†²
            
            # ç»´æŠ¤ä¸¤ä¸ªå›¾åƒå†å²çª—å£
            agentview_window = deque(maxlen=2)
            eye_in_hand_window = deque(maxlen=2)
            
            # è·å–åˆå§‹å›¾åƒ
            agentview_img = obs['agentview_image']
            eye_in_hand_img = obs['robot0_eye_in_hand_image']
            
            # ç”¨ç¬¬ä¸€å¸§å¡«å……å†å²ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰
            for _ in range(2):
                agentview_window.append(agentview_img)
                eye_in_hand_window.append(eye_in_hand_img)
            
            # è·å– proprio çŠ¶æ€ï¼ˆjoint + gripperï¼‰ï¼Œå¹¶æ¬åˆ° device / dtype
            joint_states = obs['robot0_joint_pos']
            gripper_states = obs['robot0_gripper_qpos']
            proprio_np = np.concatenate([joint_states, gripper_states], axis=-1).astype(np.float32)
            proprio = torch.from_numpy(proprio_np).to(device=device, dtype=torch.bfloat16)
            
            global_steps = 0
            done = False
            episode_done = False   # libero é£æ ¼ï¼šè¿™ä¸€æ¡ episode æ˜¯å¦å‡ºç°è¿‡ done=True
            task_success = False   # ä½¿ç”¨ info['success'] åˆ¤å®šâ€œçœŸæ­£å®Œæˆä»»åŠ¡â€
            reward = 0.0
            info = {}
            
            # ğŸ¯ é‡æ–°è§„åˆ’é¢‘ç‡ï¼ˆä¸€æ¬¡é¢„æµ‹å¤šå°‘æ­¥ï¼‰
            REPLAN_FREQ = 8
            
            while global_steps < MAX_EPISODE_STEPS and not done:
                # å‡†å¤‡å›¾åƒè¾“å…¥
                image_arrs = []
                for i in range(2):  # img_history_size = 2
                    image_arrs.append(agentview_window[i])      # å¤–éƒ¨ç›¸æœº
                    image_arrs.append(eye_in_hand_window[i])    # å³æ‰‹è…•
                    image_arrs.append(None)                     # å·¦æ‰‹è…•ï¼ˆLIBERO æ²¡æœ‰ï¼‰
                
                images = [Image.fromarray(arr) if arr is not None else None
                          for arr in image_arrs]
                
                # é¢„æµ‹åŠ¨ä½œåºåˆ—ï¼ˆæ¨ç†æ¨¡å¼ï¼Œä¸æ„å»ºè®¡ç®—å›¾ï¼‰
                with torch.inference_mode():
                    actions = policy.step(proprio, images, text_embed).squeeze(0)
                actions = actions.detach().cpu().numpy()
                
                # è°ƒè¯•ä¿¡æ¯ï¼ˆé¦–å¸§ï¼‰
                if episode == 0 and global_steps == 0:
                    print(f"\n{'='*60}")
                    print(f"ã€é¦–æ¬¡é¢„æµ‹è°ƒè¯•ä¿¡æ¯ã€‘")
                    print(f"  Proprio shape: {proprio.shape}, range: [{proprio.min().item():.4f}, {proprio.max().item():.4f}]")
                    print(f"  Actions shape: {actions.shape}")
                    print(f"  EEF vel range: [{actions[:, :6].min():.4f}, {actions[:, :6].max():.4f}]")
                    print(f"  Gripper values (first 5): {actions[:5, -1]}")
                    print(f"  Expected: gripper in {{-1, 1}}, EEF vel in [-1, 1]")
                    print(f"{'='*60}\n")
                
                # åªæ‰§è¡Œå‰ N æ­¥
                num_exec_steps = min(REPLAN_FREQ, actions.shape[0], MAX_EPISODE_STEPS - global_steps)
                
                for idx in range(num_exec_steps):
                    action = actions[idx]
                    
                    # å®‰å…¨æ£€æŸ¥
                    if np.any(np.isnan(action)) or np.any(np.isinf(action)):
                        print(f"âš ï¸  Invalid action detected at step {global_steps}, skipping...")
                        break
                    
                    obs, reward, done, info = env.step(action)
                    
                    # è®°å½•è§†é¢‘å¸§
                    video_writer.append_obs(
                        obs, 
                        done, 
                        idx=episode,
                        camera_name="agentview_image"
                    )
                    
                    # æ›´æ–°è§‚å¯Ÿçª—å£
                    agentview_window.append(obs['agentview_image'])
                    eye_in_hand_window.append(obs['robot0_eye_in_hand_image'])
                    
                    # æ›´æ–° proprio
                    joint_states = obs['robot0_joint_pos']
                    gripper_states = obs['robot0_gripper_qpos']
                    proprio_np = np.concatenate([joint_states, gripper_states], axis=-1).astype(np.float32)
                    proprio = torch.from_numpy(proprio_np).to(device=device, dtype=torch.bfloat16)
                    
                    global_steps += 1
                    
                    # è¿›åº¦ç›‘æ§ï¼ˆä»…ç¬¬ä¸€ä¸ª episodeï¼‰
                    if episode == 0 and global_steps % 50 == 0:
                        print(f"  â†’ Step {global_steps:3d}: reward={reward:.2f}")
                    
                    # åªè¦ env è¿”å› done=Trueï¼Œå°±è®¤ä¸ºè¿™ä¸€æ¡ episode ç»“æŸ
                    if done:
                        episode_done = True
                        break
                
                if done:
                    break
            
            # å¾ªç¯å¤–æ›´æ–° â€œlibero é£æ ¼æˆåŠŸè®¡æ•°â€ï¼šè¿™ä¸€æ¡ episode æ˜¯å¦ç»ˆæ­¢
            if episode_done:
                success_count += 1
            
            # å¢å¼ºçš„è¿›åº¦è¾“å‡ºï¼šstatus çœ‹çš„æ˜¯ info['success']
            status = "âœ“ SUCCESS" if episode_done else "âœ— FAILED"
            print(
                f"Trial {episode+1:3d}/{total_episodes}: {status} "
                f"| done={episode_done} info['success']={info.get('success', False)} "
                f"| steps={global_steps:3d}"
            )
        
        # VideoWriter ä¼šåœ¨é€€å‡º with å—æ—¶è‡ªåŠ¨ä¿å­˜æ‰€æœ‰è§†é¢‘
    
    env.close()
    
    # 7. è¾“å‡ºç»“æœï¼ˆsuccess_rate ä¸º 0~1ï¼‰
    success_rate = success_count / total_episodes
    print(f"\n{'='*50}")
    print(f"Task: {task_name}")
    print(f"Instruction: {instruction}")
    print(f"Total Episodes: {total_episodes}")
    print(f"Episode Done Count (libero-style): {success_count}")
    print(f"Success Rate (libero-style): {success_rate * 100:.2f}%")
    print(f"{'='*50}")
    
    if args.save_videos:
        print(f"\nğŸ“¹ Videos saved to: {video_folder}")
    
    # 8. å†™å…¥ CSVï¼ˆå¦‚æœæä¾›äº† --metrics-pathï¼‰
    if getattr(args, "metrics_path", None) is not None:
        metrics_path = args.metrics_path
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨ï¼ˆå¯èƒ½åªæœ‰æ–‡ä»¶åï¼Œæ²¡æœ‰ç›®å½•ï¼‰
        metrics_dir = os.path.dirname(metrics_path)
        if metrics_dir != "":
            os.makedirs(metrics_dir, exist_ok=True)
        
        file_exists = os.path.isfile(metrics_path) and os.path.getsize(metrics_path) > 0
        
        with open(metrics_path, "a", newline="") as f:
            writer = csv.writer(f)
            # ç¬¬ä¸€æ¬¡å†™å…¥æ—¶å†™è¡¨å¤´
            if not file_exists:
                writer.writerow([
                    "dataset_name",      # å¦‚ libero_90
                    "task_id",           # ä»»åŠ¡ id
                    "task_name",         # è§£æåçš„ä»»åŠ¡å
                    "instruction",       # è¯­è¨€æè¿°
                    "num_traj",          # episode æ•°
                    "episode_done_count",# æœ‰ done çš„ episode æ•°
                    "success_rate",      # æˆåŠŸç‡ï¼ˆ0~1ï¼Œlibero-styleï¼‰
                    "checkpoint_path",   # æ¨¡å‹è·¯å¾„
                    "video_dir",         # è§†é¢‘ç›®å½•
                ])
            
            writer.writerow([
                args.dataset_name,
                args.task_id,
                task_name,
                instruction,
                total_episodes,
                success_count,
                success_rate,
                args.pretrained_path,
                video_folder if args.save_videos else "",
            ])
        
        print(f"ğŸ“„ Metrics appended to CSV: {metrics_path}")


if __name__ == "__main__":
    main()