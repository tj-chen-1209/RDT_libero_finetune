"""
ä¼˜åŒ–ç‰ˆè¯„ä¼°è„šæœ¬ï¼šä¸€æ¬¡åŠ è½½æ¨¡å‹ï¼Œè¯„ä¼°æ‰€æœ‰ä»»åŠ¡
ç›¸æ¯”åŸç‰ˆ eval_rdt_libero_subEnv.pyï¼Œé¿å…äº†é‡å¤åŠ è½½æ¨¡å‹ï¼Œå¤§å¹…æå‡è¯„ä¼°é€Ÿåº¦
"""
import os
import csv
import random
import time

import numpy as np
import sys
import torch
import yaml
import argparse
from collections import deque
from PIL import Image

LIBERO_REPO_ROOT = "/home/zhukefei/chensiqi/LIBERO"

if LIBERO_REPO_ROOT not in sys.path:
    sys.path.insert(0, LIBERO_REPO_ROOT)

from libero.libero import get_libero_path
from libero.libero.benchmark import get_benchmark_dict
from libero.libero.envs import OffScreenRenderEnv, SubprocVectorEnv
from libero_rdt_model import create_model, RoboticDiffusionTransformerModel
from libero.libero.utils.video_utils import VideoWriter 

def load_config(config_path):
    """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def parse_args():
    parser = argparse.ArgumentParser(description="è¯„ä¼°æ‰€æœ‰ä»»åŠ¡ï¼ˆä¸€æ¬¡åŠ è½½æ¨¡å‹ï¼‰")
    
    # é…ç½®æ–‡ä»¶å‚æ•°ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    parser.add_argument("--config", type=str, default=None,
                        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆYAMLæ ¼å¼ï¼‰ï¼Œå¦‚æœæä¾›åˆ™å…¶ä»–å‚æ•°å¯é€‰")
    
    # åŸæœ‰å‚æ•°ï¼ˆå¯é€šè¿‡é…ç½®æ–‡ä»¶æˆ–å‘½ä»¤è¡ŒæŒ‡å®šï¼‰
    parser.add_argument("--num-traj", type=int, default=None, help="æ¯ä¸ªä»»åŠ¡çš„è¯„ä¼°è½¨è¿¹æ•°")
    parser.add_argument("--pretrained-path", type=str, default=None, help="é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
    parser.add_argument("--dataset-name", type=str, default=None, 
                        choices=["libero_10", "libero_90", "libero_object", "libero_spatial", "libero_goal"], 
                        help="æ•°æ®é›†åç§°")
    parser.add_argument("--lora-adapter", type=str, default=None, 
                        help="LoRA adapter è·¯å¾„ï¼ˆå¦‚æœä½¿ç”¨ LoRAï¼‰")
    parser.add_argument("--metrics-path", type=str, default=None,
                        help="è¯„ä¼°ç»“æœä¿å­˜çš„ CSV è·¯å¾„")
    parser.add_argument("--seed", type=int, default=None, 
                        help="éšæœºç§å­ï¼ˆç”¨äºå¤ç°ï¼‰")
    parser.add_argument("--video-save-interval", type=int, default=None,
                        help="è§†é¢‘ä¿å­˜é—´éš”ï¼ˆ0=ä¸ä¿å­˜ï¼Œ10=æ¯10ä¸ªä»»åŠ¡ä¿å­˜ä¸€æ¬¡ï¼‰")
    parser.add_argument("--video-root-dir", type=str, default=None,
                        help="è§†é¢‘ä¿å­˜æ ¹ç›®å½•")
    parser.add_argument("--gpu", type=int, default=None,
                        help="æŒ‡å®šä½¿ç”¨çš„ GPU ç¼–å·ï¼ˆä¾‹å¦‚ï¼š0, 1, 2...ï¼‰ï¼Œé»˜è®¤ä½¿ç”¨ cuda:0")
    
    args = parser.parse_args()
    
    # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶ï¼Œä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°
    if args.config:
        if not os.path.exists(args.config):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        
        config = load_config(args.config)
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆï¼‰
        if args.dataset_name is None:
            args.dataset_name = config.get('dataset', {}).get('name', 'libero_10')
        
        if args.pretrained_path is None:
            args.pretrained_path = config.get('model', {}).get('pretrained_path')
        
        if args.lora_adapter is None:
            args.lora_adapter = config.get('model', {}).get('lora_adapter')
        
        if args.num_traj is None:
            args.num_traj = config.get('evaluation', {}).get('num_traj', 20)
        
        if args.seed is None:
            args.seed = config.get('evaluation', {}).get('seed', 20241201)
        
        if args.video_save_interval is None:
            args.video_save_interval = config.get('evaluation', {}).get('video', {}).get('save_interval', 10)
        
        if args.video_root_dir is None:
            args.video_root_dir = config.get('evaluation', {}).get('video', {}).get('root_dir', 'outs/eval_videos')
        
        if args.gpu is None:
            args.gpu = config.get('evaluation', {}).get('gpu', 0)
        
        # è‡ªåŠ¨ç”Ÿæˆmetrics_pathï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if args.metrics_path is None:
            metrics_dir = config.get('output', {}).get('metrics_dir', 'outs/metrics')
            prefix = config.get('output', {}).get('prefix', '')
            
            # ç¡®å®šcheckpointåç§°
            if args.lora_adapter:
                checkpoint_name = os.path.basename(args.lora_adapter)
            else:
                checkpoint_name = os.path.basename(args.pretrained_path)
            
            # ç”Ÿæˆæ–‡ä»¶å
            import datetime
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{prefix}{args.dataset_name}_eval{args.num_traj}eps_{checkpoint_name}_seed{args.seed}_{timestamp}.csv"
            args.metrics_path = os.path.join(metrics_dir, filename)
    
    # æ£€æŸ¥å¿…å¡«å‚æ•°
    if args.pretrained_path is None:
        parser.error("å¿…é¡»æä¾› --pretrained-path æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š model.pretrained_path")
    
    if args.metrics_path is None:
        parser.error("å¿…é¡»æä¾› --metrics-path æˆ–ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆä¼šè‡ªåŠ¨ç”Ÿæˆï¼‰")
    
    # è®¾ç½®é»˜è®¤å€¼ï¼ˆå¦‚æœä»ä¸ºNoneï¼‰
    if args.num_traj is None:
        args.num_traj = 20
    if args.dataset_name is None:
        args.dataset_name = "libero_10"
    if args.seed is None:
        args.seed = 20241201
    if args.video_save_interval is None:
        args.video_save_interval = 10
    if args.video_root_dir is None:
        args.video_root_dir = "outs/eval_videos"
    if args.gpu is None:
        args.gpu = 0
    
    return args


def _get_obs_item(obs, key, idx):
    """å…¼å®¹ä¸¤ç§ vector obs ç»“æ„"""
    if isinstance(obs, dict):
        return obs[key][idx]
    else:
        return obs[idx][key]


def set_global_seeds(seed: int):
    """ç»Ÿä¸€æ§åˆ¶éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def get_task_name_from_bddl(bddl_file_path):
    """ä» BDDL æ–‡ä»¶è·¯å¾„æå–ä»»åŠ¡åç§°"""
    filename = os.path.basename(bddl_file_path)
    return filename.replace('.bddl', '')


def extract_instruction_from_task_name(task_name):
    """ä»ä»»åŠ¡åç§°æå–æŒ‡ä»¤"""
    if task_name[0].isupper():
        scene_pos = task_name.find("SCENE")
        if scene_pos != -1:
            if "SCENE10" in task_name:
                language_part = task_name[scene_pos + 8:]
            else:
                language_part = task_name[scene_pos + 7:]
            return language_part.replace('_', ' ')
    return task_name.replace('_', ' ')


def load_language_embedding(task_name, dataset_name, policy, device):
    """åŠ è½½è¯­è¨€åµŒå…¥"""
    lang_embed_path = os.path.join("outs/libero_embeddings", dataset_name, f"{task_name}.pt")

    if os.path.exists(lang_embed_path):
        print(f"  âœ“ åŠ è½½é¢„è®¡ç®—åµŒå…¥: {task_name}")
        lang_data = torch.load(lang_embed_path)
        if isinstance(lang_data, dict):
            embeddings = lang_data['embeddings']
        else:
            embeddings = lang_data
        
        if embeddings.dim() == 2:
            embeddings = embeddings.unsqueeze(0)
        
        return embeddings.to(device=device, dtype=torch.bfloat16)
    else:
        print(f"  âš  å®æ—¶ç¼–ç åµŒå…¥: {task_name}")
        instruction = extract_instruction_from_task_name(task_name)
        return policy.encode_instruction(instruction).to(device=device, dtype=torch.bfloat16)


def evaluate_single_task(
    task_id,
    policy,
    device,
    benchmark_dict,
    args,
    checkpoint_identifier
):
    """
    è¯„ä¼°å•ä¸ªä»»åŠ¡
    è¿”å›ï¼š(success_rate, num_success, task_name, instruction)
    """
    print(f"\n{'â”€'*80}")
    print(f"æ­£åœ¨è¯„ä¼° Task {task_id}")
    print(f"{'â”€'*80}")
    
    task_start_time = time.time()
    
    # è·å–ä»»åŠ¡ä¿¡æ¯
    task_suite = benchmark_dict[args.dataset_name]()
    task = task_suite.get_task(task_id)
    
    bddl_file = os.path.join(
        get_libero_path("bddl_files"),
        task.problem_folder,
        task.bddl_file,
    )
    
    task_name = get_task_name_from_bddl(bddl_file)
    instruction = extract_instruction_from_task_name(task_name)
    
    print(f"  ä»»åŠ¡åç§°: {task_name}")
    print(f"  æŒ‡ä»¤: {instruction}")
    
    # åŠ è½½è¯­è¨€åµŒå…¥
    text_embed = load_language_embedding(task_name, args.dataset_name, policy, device)
    
    # è·å–åˆå§‹çŠ¶æ€
    init_states = task_suite.get_task_init_states(task_id)
    num_init_states = init_states.shape[0]
    
    # åˆ›å»ºå¹¶è¡Œç¯å¢ƒ
    env_num = args.num_traj
    env_args = dict(
        bddl_file_name=bddl_file,
        camera_heights=128,
        camera_widths=128,
    )
    
    env = SubprocVectorEnv(
        [lambda: OffScreenRenderEnv(**env_args) for _ in range(env_num)]
    )
    env.seed(args.seed)
    env.reset()
    
    # åˆ†é…åˆå§‹çŠ¶æ€
    indices = np.arange(env_num) % num_init_states
    init_states_batch = init_states[indices]
    obs = env.set_init_state(init_states_batch)
    
    # å‡†å¤‡å›¾åƒå†å²å’Œ proprio
    MAX_EPISODE_STEPS = 400
    dones = [False] * env_num
    global_steps = 0
    
    agentview_windows = [deque(maxlen=2) for _ in range(env_num)]
    eye_in_hand_windows = [deque(maxlen=2) for _ in range(env_num)]
    proprios = [None for _ in range(env_num)]
    
    for i in range(env_num):
        agent_img = _get_obs_item(obs, "agentview_image", i)
        eye_img = _get_obs_item(obs, "robot0_eye_in_hand_image", i)
        
        for _ in range(2):
            agentview_windows[i].append(agent_img)
            eye_in_hand_windows[i].append(eye_img)
        
        joint_states = _get_obs_item(obs, "robot0_joint_pos", i)
        gripper_states = _get_obs_item(obs, "robot0_gripper_qpos", i)
        proprio_np = np.concatenate([joint_states, gripper_states], axis=-1).astype(np.float32)
        proprios[i] = torch.from_numpy(proprio_np).to(device=device, dtype=torch.bfloat16)
    
    # ç‰©ç†é¢„çƒ­
    for _ in range(5):
        env.step(np.zeros((env_num, 7), dtype=np.float32))
    
    # å†³å®šæ˜¯å¦ä¿å­˜è§†é¢‘
    save_videos = (args.video_save_interval > 0 and task_id % args.video_save_interval == 0)
    
    if save_videos:
        video_folder = os.path.join(
            args.video_root_dir,
            f"{args.dataset_name}_task{task_id}"
        )
        os.makedirs(video_folder, exist_ok=True)
        print(f"  ğŸ“¹ å°†ä¿å­˜è§†é¢‘åˆ°: {video_folder}")
    else:
        video_folder = "outs/videos"  # è™½ç„¶ä¸ä¿å­˜ï¼Œä½†éœ€è¦ä¸€ä¸ªè·¯å¾„
    
    num_success = 0
    
    with VideoWriter(video_folder, save_video=save_videos, fps=30, single_video=True) as video_writer:
        while (global_steps < MAX_EPISODE_STEPS) and (not all(dones)):
            actions = np.zeros((env_num, 7), dtype=np.float32)
            
            # æ¨ç†
            with torch.inference_mode():
                for i in range(env_num):
                    if dones[i]:
                        continue
                    
                    # å‡†å¤‡å›¾åƒ
                    image_arrs = []
                    for t in range(2):
                        image_arrs.append(agentview_windows[i][t])
                        image_arrs.append(eye_in_hand_windows[i][t])
                        image_arrs.append(None)
                    
                    images = [
                        Image.fromarray(arr) if arr is not None else None
                        for arr in image_arrs
                    ]
                    
                    # RDT æ¨ç†
                    action_seq = policy.step(proprios[i], images, text_embed).squeeze(0)
                    action_seq = action_seq.detach().cpu().numpy()
                    
                    if np.any(np.isnan(action_seq)) or np.any(np.isinf(action_seq)):
                        continue
                    
                    actions[i] = action_seq[0]
            
            # ç¯å¢ƒæ­¥è¿›
            obs, reward, done, info = env.step(actions)
            global_steps += 1
            
            # è®°å½•è§†é¢‘
            if save_videos:
                video_writer.append_vector_obs(
                    obs,
                    dones,
                    camera_name="agentview_image",
                    info=info
                )
            
            # æ›´æ–°è§‚æµ‹
            for i in range(env_num):
                if not dones[i]:
                    agent_img = _get_obs_item(obs, "agentview_image", i)
                    eye_img = _get_obs_item(obs, "robot0_eye_in_hand_image", i)
                    
                    agentview_windows[i].append(agent_img)
                    eye_in_hand_windows[i].append(eye_img)
                    
                    joint_states = _get_obs_item(obs, "robot0_joint_pos", i)
                    gripper_states = _get_obs_item(obs, "robot0_gripper_qpos", i)
                    proprio_np = np.concatenate([joint_states, gripper_states], axis=-1).astype(np.float32)
                    proprios[i] = torch.from_numpy(proprio_np).to(device=device, dtype=torch.bfloat16)
                    
                    if done[i]:
                        dones[i] = True
        
        # ç»Ÿè®¡æˆåŠŸæ•°
        for i in range(env_num):
            num_success += int(dones[i])
    
    env.close()
    
    # è®¡ç®—æˆåŠŸç‡
    success_rate = num_success / float(env_num)
    task_time = time.time() - task_start_time
    
    print(f"  âœ“ æˆåŠŸç‡: {success_rate * 100:.2f}% ({num_success}/{env_num})")
    print(f"  â± è€—æ—¶: {task_time:.1f}ç§’")
    
    # å†™å…¥ CSV
    write_task_result_to_csv(
        args.metrics_path,
        args.dataset_name,
        task_id,
        task_name,
        instruction,
        env_num,
        num_success,
        success_rate,
        checkpoint_identifier,
        video_folder if save_videos else ""
    )
    
    return success_rate, num_success, task_name, instruction


def write_task_result_to_csv(
    metrics_path,
    dataset_name,
    task_id,
    task_name,
    instruction,
    num_traj,
    episode_done_count,
    success_rate,
    checkpoint_path,
    video_dir
):
    """å°†å•ä¸ªä»»åŠ¡ç»“æœå†™å…¥ CSV"""
    metrics_dir = os.path.dirname(metrics_path)
    if metrics_dir != "":
        os.makedirs(metrics_dir, exist_ok=True)
    
    file_exists = os.path.isfile(metrics_path) and os.path.getsize(metrics_path) > 0
    
    with open(metrics_path, "a", newline="") as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow([
                "dataset_name",
                "task_id",
                "task_name",
                "instruction",
                "num_traj",
                "episode_done_count",
                "success_rate",
                "checkpoint_path",
                "video_dir",
            ])
        
        writer.writerow([
            dataset_name,
            task_id,
            task_name,
            instruction,
            num_traj,
            episode_done_count,
            success_rate,
            checkpoint_path,
            video_dir,
        ])


def write_average_to_csv(metrics_path, dataset_name, checkpoint_path, random_seed, run_id, num_episodes):
    """è®¡ç®—å¹¶å†™å…¥å¹³å‡æˆåŠŸç‡"""
    # è¯»å–æ‰€æœ‰ä»»åŠ¡ç»“æœ
    success_rates = []
    episode_success_counts = []
    
    with open(metrics_path, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            try:
                success_rate = float(row['success_rate'])
                episode_done = int(row['episode_done_count'])
                success_rates.append(success_rate)
                episode_success_counts.append(episode_done)
            except (ValueError, KeyError):
                continue
    
    if len(success_rates) == 0:
        return
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    total_tasks = len(success_rates)
    avg_success_rate = sum(success_rates) / total_tasks
    total_success_episodes = sum(episode_success_counts)
    total_episodes = total_tasks * num_episodes
    
    # è¿½åŠ å¹³å‡å€¼
    with open(metrics_path, 'a', newline='') as f:
        writer = csv.writer(f)
        writer.writerow([
            dataset_name,
            "AVG",
            "AVERAGE",
            f"Average success rate across all tasks (seed={random_seed})",
            total_episodes,
            total_success_episodes,
            avg_success_rate,
            checkpoint_path,
            run_id
        ])


def main():
    args = parse_args()
    
    print("="*80)
    print("RDT-LIBERO æ‰¹é‡è¯„ä¼°ï¼ˆä¼˜åŒ–ç‰ˆï¼šä¸€æ¬¡åŠ è½½æ¨¡å‹ï¼‰")
    print("="*80)
    print(f"æ•°æ®é›†: {args.dataset_name}")
    print(f"æ¯ä»»åŠ¡è¯„ä¼°è½®æ¬¡: {args.num_traj}")
    print(f"éšæœºç§å­: {args.seed}")
    print(f"ç‰©ç† GPU ç¼–å·: {args.gpu}")
    print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'not set')}")
    print(f"è§†é¢‘ä¿å­˜é—´éš”: {args.video_save_interval}")
    print("="*80)
    
    # ç¡®å®šä»»åŠ¡æ•°é‡
    if args.dataset_name == "libero_10":
        task_range = range(0, 10)
    elif args.dataset_name == "libero_90":
        task_range = range(0, 90)
    elif args.dataset_name in ["libero_spatial", "libero_object", "libero_goal"]:
        task_range = range(0, 10)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {args.dataset_name}")
    
    # è®¾ç½®éšæœºç§å­
    set_global_seeds(args.seed)
    
    # è®¾ç½®è®¾å¤‡
    # æ³¨æ„ï¼šå› ä¸ºå·²ç»è®¾ç½®äº† CUDA_VISIBLE_DEVICESï¼Œæ‰€ä»¥è¿™é‡Œä½¿ç”¨ cuda:0
    # ï¼ˆå®é™…çš„ç‰©ç†GPUå·²ç»é€šè¿‡ CUDA_VISIBLE_DEVICES æ˜ å°„äº†ï¼‰
    if torch.cuda.is_available():
        device = torch.device("cuda:0")
        print(f"\nä½¿ç”¨è®¾å¤‡: ç‰©ç† GPU {args.gpu} (æ˜ å°„ä¸º cuda:0)")
    else:
        device = torch.device("cpu")
        print(f"\nä½¿ç”¨è®¾å¤‡: CPU (CUDA ä¸å¯ç”¨)")
    
    # ======== ä¸€æ¬¡æ€§åŠ è½½æ¨¡å‹ï¼ˆå…³é”®ä¼˜åŒ–ï¼ï¼‰========
    print("\n" + "="*80)
    print("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰...")
    print("="*80)
    
    model_start_time = time.time()
    
    config_path = 'configs/base.yaml'
    with open(config_path, "r") as fp:
        config = yaml.safe_load(fp)
    
    pretrained_text_encoder_name_or_path = "google/t5-v1_1-xxl"
    pretrained_vision_encoder_name_or_path = "google/siglip-so400m-patch14-384"
    
    policy = create_model(
        args=config, 
        dtype=torch.bfloat16,
        pretrained=args.pretrained_path,
        lora_adapter_path=args.lora_adapter,
        pretrained_text_encoder_name_or_path=pretrained_text_encoder_name_or_path,
        pretrained_vision_encoder_name_or_path=pretrained_vision_encoder_name_or_path,
    )
    
    model_load_time = time.time() - model_start_time
    print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆï¼è€—æ—¶: {model_load_time:.1f}ç§’")
    
    # ç¡®å®š checkpoint æ ‡è¯†ï¼ˆç”¨äº CSV è®°å½•ï¼‰
    checkpoint_identifier = args.lora_adapter if args.lora_adapter else args.pretrained_path
    
    # è·å– benchmark
    benchmark_dict = get_benchmark_dict()
    
    # ======== å¾ªç¯è¯„ä¼°æ‰€æœ‰ä»»åŠ¡ ========
    print("\n" + "="*80)
    print(f"å¼€å§‹è¯„ä¼° {len(task_range)} ä¸ªä»»åŠ¡...")
    print("="*80)
    
    all_start_time = time.time()
    all_success_rates = []
    all_success_counts = []
    
    for task_id in task_range:
        success_rate, num_success, task_name, instruction = evaluate_single_task(
            task_id=task_id,
            policy=policy,
            device=device,
            benchmark_dict=benchmark_dict,
            args=args,
            checkpoint_identifier=checkpoint_identifier
        )
        
        all_success_rates.append(success_rate)
        all_success_counts.append(num_success)
    
    total_time = time.time() - all_start_time
    
    # ======== è®¡ç®—å¹¶ä¿å­˜ç»Ÿè®¡ç»“æœ ========
    print("\n" + "="*80)
    print("æ‰€æœ‰ä»»åŠ¡è¯„ä¼°å®Œæˆï¼æ­£åœ¨è®¡ç®—ç»Ÿè®¡ç»“æœ...")
    print("="*80)
    
    # ç”Ÿæˆ run_id
    import datetime
    run_id = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # å†™å…¥å¹³å‡å€¼
    write_average_to_csv(
        args.metrics_path,
        args.dataset_name,
        checkpoint_identifier,
        args.seed,
        run_id,
        args.num_traj
    )
    
    # æ‰“å°æ±‡æ€»
    avg_success_rate = sum(all_success_rates) / len(all_success_rates)
    total_success = sum(all_success_counts)
    total_episodes = len(task_range) * args.num_traj
    
    print("\n" + "â•”" + "â•" * 78 + "â•—")
    print("â•‘" + " " * 28 + "è¯„ä¼°ç»“æœæ±‡æ€»" + " " * 38 + "â•‘")
    print("â• " + "â•" * 78 + "â•£")
    print(f"â•‘ æ•°æ®é›†           : {args.dataset_name:<58} â•‘")
    print(f"â•‘ Checkpoint       : {os.path.basename(checkpoint_identifier):<58} â•‘")
    print(f"â•‘ éšæœºç§å­         : {args.seed:<58} â•‘")
    print("â• " + "â•" * 78 + "â•£")
    print(f"â•‘ è¯„ä¼°ä»»åŠ¡æ€»æ•°     : {len(task_range):<58} â•‘")
    print(f"â•‘ æ¯ä»»åŠ¡è¯„ä¼°è½®æ¬¡   : {args.num_traj:<58} â•‘")
    print(f"â•‘ æ€»è¯„ä¼°è½®æ¬¡       : {total_episodes:<58} â•‘")
    print("â• " + "â•" * 78 + "â•£")
    print(f"â•‘ âœ“ å¹³å‡æˆåŠŸç‡     : {avg_success_rate * 100:>6.2f}%{' ' * 50} â•‘")
    print(f"â•‘ âœ“ æˆåŠŸè½®æ¬¡       : {total_success}/{total_episodes}{' ' * (52 - len(str(total_success)) - len(str(total_episodes)))} â•‘")
    print("â• " + "â•" * 78 + "â•£")
    print(f"â•‘ â± æ¨¡å‹åŠ è½½æ—¶é—´   : {model_load_time:>6.1f}ç§’{' ' * 48} â•‘")
    print(f"â•‘ â± è¯„ä¼°æ€»æ—¶é—´     : {total_time:>6.1f}ç§’{' ' * 48} â•‘")
    print(f"â•‘ â± å¹³å‡æ¯ä»»åŠ¡     : {total_time/len(task_range):>6.1f}ç§’{' ' * 48} â•‘")
    print("â•š" + "â•" * 78 + "â•")
    
    print(f"\nâœ… è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {args.metrics_path}")
    print(f"ğŸ“Š æ€»è€—æ—¶: {model_load_time + total_time:.1f}ç§’ (æ¨¡å‹åŠ è½½ {model_load_time:.1f}ç§’ + è¯„ä¼° {total_time:.1f}ç§’)")
    print("\nç›¸æ¯”åŸç‰ˆè„šæœ¬ï¼ŒèŠ‚çœäº†çº¦ {:.1f} ç§’çš„é‡å¤æ¨¡å‹åŠ è½½æ—¶é—´ï¼".format(model_load_time * (len(task_range) - 1)))


if __name__ == "__main__":
    main()

