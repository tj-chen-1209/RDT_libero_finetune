import os
import fnmatch
import json

import h5py
import yaml
import numpy as np

from configs.state_vec import STATE_VEC_IDX_MAPPING


class HDF5LiberoSFTDataset:
    """
    This class is used to sample episodes from the embododiment dataset
    stored in HDF5.
    """

    def __init__(self, use_cache=True) -> None:
        # [Modify] The path to the HDF5 dataset directory
        # Each HDF5 file contains one episode 
        # TODO: change the dataset name
        HDF5_DIR = "data/datasets/libero_spatial/"
        self.DATASET_NAME = "libero_spatial"
        self.use_cache = use_cache

        self.file_paths = []
        for root, _, files in os.walk(HDF5_DIR):
            for filename in fnmatch.filter(files, '*.hdf5'):
                file_path = os.path.join(root, filename)
                self.file_paths.append(file_path)

        # Load the config
        with open('configs/base.yaml', 'r') as file:
            config = yaml.safe_load(file)
        self.CHUNK_SIZE = config['common']['action_chunk_size']
        self.IMG_HISORY_SIZE = config['common']['img_history_size']
        self.STATE_DIM = config['common']['state_dim']

        # Get each episode's len
        '''
        episode_lens = []
        for file_path in self.file_paths:
            valid, res = self.parse_hdf5_file_state_only(file_path)
            _len = res['state'].shape[0] if valid else 0
            episode_lens.append(_len)
        self.episode_sample_weights = np.array(
            episode_lens) / np.sum(episode_lens)
        '''
        # siqi change
        # self.episode_paths = []  # ç”¨äºå­˜å‚¨æ¯ä¸ª demo_X è·¯å¾„
        # for file_path in self.file_paths:
        #     with h5py.File(file_path, 'r') as f:
        #         demo_keys = list(f['data'].keys())  # è·å– demo_X åç§°
        #         for demo_key in demo_keys:
        #             demo = f['data'][demo_key]
        #             num_steps = demo['actions'].shape[0]

        #             if num_steps >= 128:  # ä¸¢å¼ƒå¤ªçŸ­çš„ demo
        #                 self.episode_paths.append((file_path, demo_key, num_steps))
        # episode_lens = [episode[2]
        #                 for episode in self.episode_paths]  # è·å–æ¯ä¸ª demo_X çš„é•¿åº¦
        # self.episode_sample_weights = np.array(
        #     episode_lens) / np.sum(episode_lens)
        # åªè¾“å…¥demo_0çš„è·¯å¾„
        self.episode_paths = []  # ç”¨äºå­˜å‚¨æ¯ä¸ª demo_0 çš„è·¯å¾„
        for file_path in self.file_paths:
            with h5py.File(file_path, 'r') as f:
                # ç¡®ä¿ç»“æ„é‡Œæœ‰ data/demo_0
                if 'data' not in f:
                    print(f"File {file_path} does not have data")
                    continue
                if 'demo_0' not in f['data']:
                    print(f"File {file_path} does not have demo_0")
                    continue

                demo_key = 'demo_0'
                demo = f['data'][demo_key]
                num_steps = demo['actions'].shape[0]

                # if num_steps >= 128:  # ä¸¢å¼ƒå¤ªçŸ­çš„ demo
                self.episode_paths.append((file_path, demo_key, num_steps))
        episode_lens = [ep[2] for ep in self.episode_paths]
        self.episode_sample_weights = np.array(episode_lens) / np.sum(episode_lens)
        
        # ğŸš€ å†…å­˜ç¼“å­˜ï¼šé¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°RAMï¼ˆæ•°æ®é›†å°äº200MBï¼Œå†…å­˜å……è¶³ï¼‰
        self.cache = {}
        if self.use_cache:
            print(f"[Memory Cache] Preloading {len(self.episode_paths)} episodes to RAM...")
            for idx, (file_path, demo_key, _) in enumerate(self.episode_paths):
                with h5py.File(file_path, 'r') as f:
                    demo = f['data'][demo_key]
                    # é¢„åŠ è½½æ‰€æœ‰éœ€è¦çš„æ•°æ®
                    self.cache[file_path] = {
                        'joint_states': demo['obs']['joint_states'][:],
                        'gripper_states': demo['obs']['gripper_states'][:],
                        'agentview_rgb': demo['obs']['agentview_rgb'][:],
                        'eye_in_hand_rgb': demo['obs']['eye_in_hand_rgb'][:],
                        'actions': demo['actions'][:],
                    }
                if (idx + 1) % 5 == 0 or idx == len(self.episode_paths) - 1:
                    print(f"  Loaded {idx + 1}/{len(self.episode_paths)} episodes")
            
            # è®¡ç®—å®é™…å ç”¨çš„å†…å­˜
            total_size = sum(
                sum(arr.nbytes for arr in cache_data.values())
                for cache_data in self.cache.values()
            )
            print(f"[Memory Cache] âœ… Preloaded {len(self.cache)} episodes ({total_size/1024/1024:.1f}MB)")
            print(f"[Memory Cache] Training will be significantly faster (no disk I/O)!")

    def __len__(self):
        return len(self.episode_paths)

    def get_dataset_name(self):
        return self.DATASET_NAME

    def get_item(self, index: int = None, state_only=False):
        """Get a training sample at a random timestep.

        Args:
            index (int, optional): the index of the episode.
                If not provided, a random episode will be selected.
            state_only (bool, optional): Whether to return only the state.
                In this way, the sample will contain a complete trajectory rather
                than a single timestep. Defaults to False.

        Returns:
           sample (dict): a dictionary containing the training sample.
        """
        '''
        while True:
            if index is None:
                file_path = np.random.choice(
                    self.file_paths, p=self.episode_sample_weights)
            else:
                file_path = self.file_paths[index]
            valid, sample = self.parse_hdf5_file(file_path) \
                if not state_only else self.parse_hdf5_file_state_only(file_path)
            if valid:
                return sample
            else:
                index = np.random.randint(0, len(self.file_paths))
        '''
        while True:
            if index is None:
                demo_idx = np.random.choice(
                    len(self.episode_paths), p=self.episode_sample_weights)
                demo_path = self.episode_paths[demo_idx]
            else:
                demo_path = self.episode_paths[index]

            file_path, demo_key, _ = demo_path  # è·å–æ–‡ä»¶è·¯å¾„å’Œ demo_key
            valid, sample = self.parse_hdf5_file(file_path, demo_key) \
                if not state_only else self.parse_hdf5_file_state_only(file_path, demo_key)
            if valid:
                return sample
            else:
                index = np.random.randint(0, len(self.episode_paths))

    def parse_hdf5_file(self, file_path, demo_key):
        """[Modify] Parse a hdf5 file (or cache) to generate a training sample at
            a random timestep.

        Args:
            file_path (str): the path to the hdf5 file
            demo_key (str): the key of the demo 
        Returns:
            valid (bool): whether the episode is valid, which is useful for filtering.
                If False, this episode will be dropped.
            dict: a dictionary containing the training sample,
                {
                    "meta": {
                        "dataset_name": str,    # the name of your dataset.
                        "#steps": int,          # the number of steps in the episode,
                                                # also the total timesteps.
                        "instruction": str      # the language instruction for this episode.
                    },                           
                    "step_id": int,             # the index of the sampled step,
                                                # also the timestep t.
                    "state": ndarray,           # state[t], (1, STATE_DIM).
                    "state_std": ndarray,       # std(state[:]), (STATE_DIM,).
                    "state_mean": ndarray,      # mean(state[:]), (STATE_DIM,).
                    "state_norm": ndarray,      # norm(state[:]), (STATE_DIM,).
                    "actions": ndarray,         # action[t:t+CHUNK_SIZE], (CHUNK_SIZE, STATE_DIM).
                    "state_indicator", ndarray, # indicates the validness of each dim, (STATE_DIM,).
                    "cam_high": ndarray,        # external camera image, (IMG_HISORY_SIZE, H, W, 3)
                                                # or (IMG_HISORY_SIZE, 0, 0, 0) if unavailable.
                    "cam_high_mask": ndarray,   # indicates the validness of each timestep, (IMG_HISORY_SIZE,) boolean array.
                                                # For the first IMAGE_HISTORY_SIZE-1 timesteps, the mask should be False.
                    "cam_left_wrist": ndarray,  # left wrist camera image, (IMG_HISORY_SIZE, H, W, 3).
                                                # or (IMG_HISORY_SIZE, 0, 0, 0) if unavailable.
                    "cam_left_wrist_mask": ndarray,
                    "cam_right_wrist": ndarray, # right wrist camera image, (IMG_HISORY_SIZE, H, W, 3).
                                                # or (IMG_HISORY_SIZE, 0, 0, 0) if unavailable.
                                                # If only one wrist, make it right wrist, plz.
                    "cam_right_wrist_mask": ndarray
                } or None if the episode is invalid.
        """
        # ğŸš€ ä»ç¼“å­˜æˆ–æ–‡ä»¶è¯»å–æ•°æ®
        if self.use_cache and file_path in self.cache:
            # ä»å†…å­˜ç¼“å­˜è¯»å–ï¼ˆå¿«é€Ÿï¼‰
            cached_data = self.cache[file_path]
            joint_states = cached_data['joint_states']
            gripper_states = cached_data['gripper_states']
            agentview_rgb = cached_data['agentview_rgb']
            eye_in_hand_rgb = cached_data['eye_in_hand_rgb']
            actions_data = cached_data['actions']
            
            # Concatenate joint states and gripper states 7DoF+2gripper
            qpos = np.concatenate([joint_states, gripper_states], axis=1)
        else:
            # ä»HDF5æ–‡ä»¶è¯»å–ï¼ˆæ…¢é€Ÿï¼‰
            with h5py.File(file_path, 'r') as f:
                # LIBERO dataset structure: data/demo_X/
                demo = f['data'][demo_key]
                joint_states = demo['obs']['joint_states'][:]
                gripper_states = demo['obs']['gripper_states'][:]
                agentview_rgb = demo['obs']['agentview_rgb'][:]
                eye_in_hand_rgb = demo['obs']['eye_in_hand_rgb'][:]
                actions_data = demo['actions'][:]

                # Concatenate joint states and gripper states 7DoF+2gripper
                qpos = np.concatenate([joint_states, gripper_states], axis=1)
        
        num_steps = qpos.shape[0]
        # [Optional] We drop too-short episode
        # if num_steps < 128:
        #     return False, None

        # [Optional] We skip the first few still steps TODO
        EPS = 1e-2
        # Get the idx of the first qpos whose delta exceeds the threshold
        qpos_delta = np.abs(qpos - qpos[0:1])
        indices = np.where(np.any(qpos_delta > EPS, axis=1))[0]
        if len(indices) > 0:
            first_idx = indices[0]
        else:
            raise ValueError("Found no qpos that exceeds the threshold.")

        # We randomly sample a timestep TODO
        step_id = np.random.randint(first_idx-1, num_steps)

        # TODO: add instruction è¿™ä¸ªåœ°æ–¹è¦è·Ÿ eval å¯¹é½
        def extract_instruction(file_path):
            """
            ä» HDF5 æ–‡ä»¶åä¸­æå–ä»»åŠ¡æŒ‡ä»¤
            
            Args:
                file_path: HDF5 æ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚ï¼š
                    "KITCHEN_SCENE3_turn_on_the_stove_and_put_the_moka_pot_on_it_demo.hdf5"
                    "pick_up_the_black_bowl_and_place_it_on_the_plate_demo.hdf5"
            
            Returns:
                str: æå–çš„æŒ‡ä»¤ï¼Œä¾‹å¦‚ï¼š
                    "turn on the stove and put the moka pot on it"
                    "pick up the black bowl and place it on the plate"
            """
            # è·å–æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„å’Œæ‰©å±•åï¼‰
            task_name = os.path.basename(file_path).replace('_demo.hdf5', '')
             # å¦‚æœæ–‡ä»¶åä»¥å¤§å†™å­—æ¯å¼€å¤´ï¼ˆå¦‚ KITCHEN_SCENE3_...ï¼‰
            if task_name and task_name[0].isupper():
                # æŸ¥æ‰¾ SCENE çš„ä½ç½®
                scene_pos = task_name.find("SCENE")
                if scene_pos != -1:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ SCENE10ï¼ˆéœ€è¦è·³è¿‡ 8 ä¸ªå­—ç¬¦ï¼šSCENE10_ï¼‰
                    if "SCENE10" in task_name:
                        # ä» SCENE10_ ä¹‹åå¼€å§‹æå–
                        language_part = task_name[scene_pos + 8:]
                    else:
                        # ä» SCENE#_ ä¹‹åå¼€å§‹æå–ï¼ˆè·³è¿‡ 7 ä¸ªå­—ç¬¦ï¼šSCENE + æ•°å­— + _ï¼‰
                        # ä¾‹å¦‚ï¼šSCENE3_ -> è·³è¿‡ 7 ä¸ªå­—ç¬¦
                        language_part = task_name[scene_pos + 7:]
                    
                    # å°†ä¸‹åˆ’çº¿æ›¿æ¢ä¸ºç©ºæ ¼
                    instruction = language_part.replace('_', ' ')
                else:
                    # æ²¡æœ‰æ‰¾åˆ° SCENEï¼Œç›´æ¥æ›¿æ¢æ‰€æœ‰ä¸‹åˆ’çº¿
                    instruction = task_name.replace('_', ' ')
            else:
                # æ–‡ä»¶åä¸ä»¥å¤§å†™å­—æ¯å¼€å¤´ï¼Œç›´æ¥æ›¿æ¢æ‰€æœ‰ä¸‹åˆ’çº¿
                instruction = task_name.replace('_', ' ')
            
            return instruction.strip()
        
        # embeddied instruction
        task_name = os.path.basename(file_path).replace('_demo.hdf5', '')
        lang_embed_path = os.path.join("outs/libero_embeddings", self.DATASET_NAME, task_name + ".pt")

        if os.path.exists(lang_embed_path):
            instruction = lang_embed_path
        else:
            instruction = extract_instruction(file_path)
        # You can also use precomputed language embeddings (recommended)
        # instruction = "path/to/lang_embed.pt"

        # Assemble the meta
        meta = {
            "dataset_name": self.DATASET_NAME,
            "#steps": num_steps,
            "step_id": step_id,
            "instruction": instruction
        }

        # Max-min normalization for the gripper states (last 2 dimensions) TODO: max-min qpos correct
        qpos_min = -0.04245
        qpos_max = 0.05185
        qpos[..., -2:] = (qpos[..., -2:] - qpos_min) / \
            (qpos_max - qpos_min)

        # Extract actions: 6D EEF velocities + 1D gripper velocity
        target_qpos = actions_data[step_id:step_id+self.CHUNK_SIZE]

        # Parse the state and action
        state = qpos[step_id:step_id+1]
        state_std = np.std(qpos, axis=0)
        state_mean = np.mean(qpos, axis=0)
        state_norm = np.sqrt(np.mean(qpos**2, axis=0))
        actions = target_qpos
        if actions.shape[0] < self.CHUNK_SIZE:
            # Pad the actions using the last action
            actions = np.concatenate([
                actions,
                np.tile(actions[-1:],
                        (self.CHUNK_SIZE-actions.shape[0], 1))
            ], axis=0)

        # Fill the state/action into the unified vector
        def fill_in_state(values):
            # Target indices corresponding to your state space
            # In this example: 7 joints + 2 gripper for each arm
            UNI_STATE_INDICES = [
                STATE_VEC_IDX_MAPPING[f"arm_joint_{i}_pos"] for i in range(7)
            ] + [
                STATE_VEC_IDX_MAPPING[f"gripper_joint_{i}_pos"] for i in range(2)
            ]
            uni_vec = np.zeros(values.shape[:-1] + (self.STATE_DIM,))
            uni_vec[..., UNI_STATE_INDICES] = values
            return uni_vec
        state = fill_in_state(state)
        state_indicator = fill_in_state(np.ones_like(state_std))
        state_std = fill_in_state(state_std)
        state_mean = fill_in_state(state_mean)
        state_norm = fill_in_state(state_norm)

        # Fill the action into the unified vector éƒ½è®¤ä¸ºæ˜¯velocity
        def fill_in_action(values):
            UNI_ACTION_INDICES = [
                STATE_VEC_IDX_MAPPING["eef_vel_x"],
                STATE_VEC_IDX_MAPPING["eef_vel_y"],
                STATE_VEC_IDX_MAPPING["eef_vel_z"],
                STATE_VEC_IDX_MAPPING["eef_angular_vel_roll"],
                STATE_VEC_IDX_MAPPING["eef_angular_vel_pitch"],
                STATE_VEC_IDX_MAPPING["eef_angular_vel_yaw"],
            ] + [
                STATE_VEC_IDX_MAPPING["gripper_open"]
            ]
            uni_vec = np.zeros(values.shape[:-1] + (self.STATE_DIM,))
            uni_vec[..., UNI_ACTION_INDICES] = values
            return uni_vec
        actions = fill_in_action(actions)

        # Parse the images liberoçš„å›¾ç‰‡æ˜¯numpyæ•°ç»„
        def parse_img(img_array):
            # img_arrayæ˜¯é¢„åŠ è½½çš„å®Œæ•´å›¾åƒåºåˆ—
            if img_array is None or img_array.shape[1] == 0:
                return np.zeros((self.IMG_HISORY_SIZE, 0, 0, 0))
            imgs = []
            for i in range(max(step_id-self.IMG_HISORY_SIZE+1, 0), step_id+1):
                img = img_array[i]
                imgs.append(img)
            imgs = np.stack(imgs)
            if imgs.shape[0] < self.IMG_HISORY_SIZE:
                # Pad the images using the first image
                imgs = np.concatenate([
                    np.tile(imgs[:1], (self.IMG_HISORY_SIZE -
                            imgs.shape[0], 1, 1, 1)),
                    imgs
                ], axis=0)
            return imgs

        # For step_id = first_idx - 1, the valid_len should be one
        valid_len = min(step_id - (first_idx - 1) +
                        1, self.IMG_HISORY_SIZE)
        cam_high_mask = np.array(
            [False] * (self.IMG_HISORY_SIZE - valid_len) +
            [True] * valid_len
        )

        # `cam_high` is the external camera image
        cam_high = parse_img(agentview_rgb)
        cam_high_mask = cam_high_mask if cam_high.shape[1] > 0 else np.zeros(
            self.IMG_HISORY_SIZE, dtype=bool)

        # Left wrist camera is not available in LIBERO
        cam_left_wrist = np.zeros((self.IMG_HISORY_SIZE, 0, 0, 0))
        cam_left_wrist_mask = np.zeros(self.IMG_HISORY_SIZE, dtype=bool)

        # Right wrist camera (eye-in-hand)
        cam_right_wrist = parse_img(eye_in_hand_rgb)
        cam_right_wrist_mask = cam_high_mask.copy(
        ) if cam_right_wrist.shape[1] > 0 else np.zeros(self.IMG_HISORY_SIZE, dtype=bool)

        # Return the resulting sample
        # For unavailable images, return zero-shape arrays, i.e., (IMG_HISORY_SIZE, 0, 0, 0)
        # E.g., return np.zeros((self.IMG_HISORY_SIZE, 0, 0, 0)) for the key "cam_left_wrist",
        # if the left-wrist camera is unavailable on your robot
        return True, {
            "meta": meta,
            "state": state,
            "state_std": state_std,
            "state_mean": state_mean,
            "state_norm": state_norm,
            "actions": actions,
            "state_indicator": state_indicator,
            "cam_high": cam_high,
            "cam_high_mask": cam_high_mask,
            "cam_left_wrist": cam_left_wrist,
            "cam_left_wrist_mask": cam_left_wrist_mask,
            "cam_right_wrist": cam_right_wrist,
            "cam_right_wrist_mask": cam_right_wrist_mask
        }

    def parse_hdf5_file_state_only(self, file_path, demo_key):
        """[Modify] Parse a hdf5 file (or cache) to generate a state trajectory.

        Args:
            file_path (str): the path to the hdf5 file
            demo_key (str): the key of the demo 
        Returns:
            valid (bool): whether the episode is valid, which is useful for filtering.
                If False, this episode will be dropped.
            dict: a dictionary containing the training sample,
                {
                    "state": ndarray,           # state[:], (T, STATE_DIM).
                    "action": ndarray,          # action[:], (T, STATE_DIM).
                } or None if the episode is invalid.
        """
        # ğŸš€ ä»ç¼“å­˜æˆ–æ–‡ä»¶è¯»å–æ•°æ®
        if self.use_cache and file_path in self.cache:
            # ä»å†…å­˜ç¼“å­˜è¯»å–ï¼ˆå¿«é€Ÿï¼‰
            cached_data = self.cache[file_path]
            joint_states = cached_data['joint_states']
            gripper_states = cached_data['gripper_states']
            actions = cached_data['actions']
            # Concatenate joint states and gripper states 7DoF+2gripper
            qpos = np.concatenate([joint_states, gripper_states], axis=1)
        else:
            # ä»HDF5æ–‡ä»¶è¯»å–ï¼ˆæ…¢é€Ÿï¼‰
            with h5py.File(file_path, 'r') as f:
                # LIBERO dataset structure: data/demo_X/
                demo = f['data'][demo_key]
                joint_states = demo['obs']['joint_states'][:]
                gripper_states = demo['obs']['gripper_states'][:]
                actions = demo['actions'][:]
                # Concatenate joint states and gripper states 7DoF+2gripper
                qpos = np.concatenate([joint_states, gripper_states], axis=1)
        num_steps = qpos.shape[0]
        # [Optional] We drop too-short episode
        # if num_steps < 128:
        #     return False, None

        # [Optional] We skip the first few still steps
        EPS = 1e-2
        # Get the idx of the first qpos whose delta exceeds the threshold
        qpos_delta = np.abs(qpos - qpos[0:1])
        indices = np.where(np.any(qpos_delta > EPS, axis=1))[0]
        if len(indices) > 0:
            first_idx = indices[0]
        else:
            raise ValueError("Found no qpos that exceeds the threshold.")

        # Max-min normalization for the gripper states
        qpos_min = -0.04245
        qpos_max = 0.05185
        qpos[..., -2:] = (qpos[..., -2:] - qpos_min) / \
            (qpos_max - qpos_min)

        target_qpos = actions

        # Parse the state and action
        state = qpos[first_idx-1:]
        action = target_qpos[first_idx-1:]

        # Fill the state/action into the unified vector
        def fill_in_state(values):
            # Target indices corresponding to your state space
            # In this example: 7 joints + 2 gripper for each arm
            UNI_STATE_INDICES = [
                STATE_VEC_IDX_MAPPING[f"arm_joint_{i}_pos"] for i in range(7)
            ] + [
                STATE_VEC_IDX_MAPPING[f"gripper_joint_{i}_pos"] for i in range(2)
            ]
            uni_vec = np.zeros(values.shape[:-1] + (self.STATE_DIM,))
            uni_vec[..., UNI_STATE_INDICES] = values
            return uni_vec

        def fill_in_action(values):
            UNI_ACTION_INDICES = [
                STATE_VEC_IDX_MAPPING["eef_vel_x"],
                STATE_VEC_IDX_MAPPING["eef_vel_y"],
                STATE_VEC_IDX_MAPPING["eef_vel_z"],
                STATE_VEC_IDX_MAPPING["eef_angular_vel_roll"],
                STATE_VEC_IDX_MAPPING["eef_angular_vel_pitch"],
                STATE_VEC_IDX_MAPPING["eef_angular_vel_yaw"],
            ] + [
                STATE_VEC_IDX_MAPPING["gripper_open"]
            ]
            uni_vec = np.zeros(values.shape[:-1] + (self.STATE_DIM,))
            uni_vec[..., UNI_ACTION_INDICES] = values
            return uni_vec

        state = fill_in_state(state)
        action = fill_in_action(action)

        # Return the resulting sample
        return True, {
            "state": state,
            "action": action
        }


if __name__ == "__main__":
    ds = HDF5LiberoSFTDataset()
    for i in range(len(ds)):
        print(f"Processing episode {i}/{len(ds)}...")
        ds.get_item(i)
