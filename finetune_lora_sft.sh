export run_id=$(date +%Y%m%d_%H%M%S)
export NCCL_IB_HCA=mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1,mlx5_7:1,mlx5_8:1,mlx5_9:1
export NCCL_IB_DISABLE=0
export NCCL_SOCKET_IFNAME=bond0
export NCCL_DEBUG=INFO
export NCCL_NVLS_ENABLE=0
export DS_BUILD_EVOFORMER_ATTN=0
# export CUDA_VISIBLE_DEVICES="1,2,3,4,5,6,7"
# export CUDA_VISIBLE_DEVICES=0


export TEXT_ENCODER_NAME="google/t5-v1_1-xxl"
export VISION_ENCODER_NAME="google/siglip-so400m-patch14-384"
export CFLAGS="-I/usr/include"
export LDFLAGS="-L/usr/lib/x86_64-linux-gnu"

#========================================================================
# è®­ç»ƒé…ç½®ï¼šæ•°æ®é›†å’Œå¾®è°ƒæ–¹æ³•
#========================================================================
dataset_name="libero_spatial"      # æ•°æ®é›†: libero_10, libero_spatial, libero_object, libero_goal, libero_90
finetune_method="LoRA"             # å¾®è°ƒæ–¹æ³•: LoRA (å‚æ•°é«˜æ•ˆ) æˆ– Full (å…¨å‚æ•°)
model_size="1B"                    # æ¨¡åž‹å¤§å°: 1B
lora_rank=32                       # LoRA rank
lora_alpha=64                      # LoRA alpha
seed=42                            # éšæœºç§å­ï¼ˆç”¨äºŽå¯å¤çŽ°æ€§ï¼‰

# åŸºç¡€æ¨¡åž‹è·¯å¾„ï¼ˆç”¨äºŽ LoRA è®­ç»ƒçš„ base modelï¼‰
base_model_name="lora-libero_basemodel"  # åŸºç¡€æ¨¡åž‹æ ‡è¯†ï¼šscratch(ä»Žå¤´), lora-ckpt20k(ä»ŽLoRA 20kæ­¥ç»§ç»­)
BASE_MODEL_PATH="./checkpoints/rdt-finetune-1b-20251119_122234/checkpoint-65000" 

# LoRA checkpoint æ¢å¤è·¯å¾„ï¼ˆå¦‚æžœè¦ä»Žä¹‹å‰çš„ LoRA checkpoint ç»§ç»­è®­ç»ƒï¼‰
# RESUME_LORA_CHECKPOINT="./checkpoints/RDT-1B-LoRA-libero_spatial-from_spatial-ckpt20k-r32a64-20251127_235342/checkpoint-25000"
RESUME_LORA_CHECKPOINT=""


export WANDB_PROJECT="rdt_libero_sft_lora_csq"

#========================================================================
# LoRA å¾®è°ƒæ¨¡å¼ï¼ˆæŽ¨èç”¨äºŽå¿«é€Ÿå®žéªŒå’Œèµ„æºå—é™åœºæ™¯ï¼‰
# ========================================================================
# ä¼˜åŠ¿ï¼š
#   - æ˜¾å­˜å ç”¨å°‘ï¼ˆçº¦èŠ‚çœ50%ï¼‰
#   - è®­ç»ƒé€Ÿåº¦å¿«ï¼ˆå¿«1.5-2å€ï¼‰
#   - æƒé‡æ–‡ä»¶å°ï¼ˆå‡ MB vs å‡ GBï¼‰
#   - ä¾¿äºŽç‰ˆæœ¬ç®¡ç†å’Œåˆ†äº«
#========================================================================

# ç”Ÿæˆæ¸…æ™°çš„è¾“å‡ºæ–‡ä»¶å¤¹åç§°
# æ ¼å¼: RDT-{model_size}-{method}-{dataset}-from_{base_model}-r{rank}a{alpha}-{timestamp}
if [ -n "$RESUME_LORA_CHECKPOINT" ]; then
    # ä»Ž LoRA checkpoint ç»§ç»­è®­ç»ƒ
    export LORA_OUTPUT_DIR="./checkpoints/RDT-${model_size}-${finetune_method}-${dataset_name}-from_${base_model_name}-r${lora_rank}a${lora_alpha}-${run_id}"
else
    # ä»Žå¤´è®­ç»ƒ
    export LORA_OUTPUT_DIR="./checkpoints/RDT-${model_size}-${finetune_method}-${dataset_name}-r${lora_rank}a${lora_alpha}-${run_id}"
fi

#========================================================================
# æ‰“å°è®­ç»ƒé…ç½®
#========================================================================
echo "=========================================================================="
echo "ðŸš€ RDT LoRA å¾®è°ƒè®­ç»ƒ"
echo "=========================================================================="
echo "ðŸ“Š æ•°æ®é›†:        ${dataset_name}"
echo "ðŸ”§ å¾®è°ƒæ–¹æ³•:      ${finetune_method}"
echo "ðŸ“¦ æ¨¡åž‹å¤§å°:      ${model_size}"
echo "ðŸ“‚ Base Model:   ${BASE_MODEL_PATH}"
if [ -n "$RESUME_LORA_CHECKPOINT" ]; then
    echo "ðŸ”„ æ¢å¤è®­ç»ƒ:      ${base_model_name}"
    echo "ðŸ“¥ LoRA Ckpt:    ${RESUME_LORA_CHECKPOINT}"
else
    echo "ðŸ†• è®­ç»ƒæ¨¡å¼:      ä»Žå¤´å¼€å§‹ LoRA è®­ç»ƒ"
fi
echo "ðŸŽ¯ LoRA Rank:     ${lora_rank}"
echo "ðŸŽ¯ LoRA Alpha:    ${lora_alpha}"
echo "ðŸŒ± éšæœºç§å­:      ${seed}"
echo "ðŸ“ è¾“å‡ºç›®å½•:      ${LORA_OUTPUT_DIR}"
echo "â° è¿è¡Œæ—¶é—´æˆ³:    ${run_id}"
echo "=========================================================================="

if [ ! -d "$LORA_OUTPUT_DIR" ]; then
    mkdir -p "$LORA_OUTPUT_DIR"
    echo "âœ… è¾“å‡ºæ–‡ä»¶å¤¹å·²åˆ›å»º: '$LORA_OUTPUT_DIR'"
else
    echo "âš ï¸  è¾“å‡ºæ–‡ä»¶å¤¹å·²å­˜åœ¨: '$LORA_OUTPUT_DIR'"
fi

# ä¿å­˜è®­ç»ƒé…ç½®åˆ°è¾“å‡ºç›®å½•
cat > "$LORA_OUTPUT_DIR/training_config.txt" << EOF
è®­ç»ƒé…ç½®ä¿¡æ¯
=====================================
æ•°æ®é›†:          ${dataset_name}
å¾®è°ƒæ–¹æ³•:        ${finetune_method}
æ¨¡åž‹å¤§å°:        ${model_size}
Base Model:      ${BASE_MODEL_PATH}
æ¢å¤è®­ç»ƒ:        ${RESUME_LORA_CHECKPOINT:-ä»Žå¤´å¼€å§‹}
LoRA Rank:       ${lora_rank}
LoRA Alpha:      ${lora_alpha}
LoRA Dropout:    0.1
éšæœºç§å­:        ${seed}
è®­ç»ƒæ‰¹æ¬¡å¤§å°:    32
å­¦ä¹ çŽ‡:          1e-4
æ··åˆç²¾åº¦:        bf16
æœ€å¤§è®­ç»ƒæ­¥æ•°:    200000
è¿è¡Œæ—¶é—´æˆ³:      ${run_id}
=====================================
EOF

echo ""
echo "ðŸ”„ å¼€å§‹è®­ç»ƒ..."
echo ""

# æž„å»º deepspeed å‘½ä»¤
RESUME_ARG=""
if [ -n "$RESUME_LORA_CHECKPOINT" ]; then
    echo "ðŸ”„ ä»Ž LoRA checkpoint æ¢å¤è®­ç»ƒ: ${RESUME_LORA_CHECKPOINT}"
    
    # ç”±äºŽè®­ç»ƒä»£ç çš„é™åˆ¶ï¼Œéœ€è¦å°† checkpoint å¤åˆ¶åˆ°æ–°çš„è¾“å‡ºç›®å½•
    resume_basename=$(basename "$RESUME_LORA_CHECKPOINT")
    target_checkpoint="$LORA_OUTPUT_DIR/$resume_basename"
    
    if [ ! -d "$target_checkpoint" ]; then
        echo "ðŸ“‹ å¤åˆ¶ checkpoint åˆ°è¾“å‡ºç›®å½•: $resume_basename"
        rsync -a --info=progress2 "$RESUME_LORA_CHECKPOINT" "$LORA_OUTPUT_DIR/"
        echo "âœ… Checkpoint å¤åˆ¶å®Œæˆ"
    else
        echo "âœ“ Checkpoint å·²å­˜åœ¨äºŽè¾“å‡ºç›®å½•"
    fi
    
    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ˆåªä¼  basenameï¼‰
    RESUME_ARG="--resume_from_checkpoint=$resume_basename"
else
    echo "ðŸ†• ä»Žå¤´å¼€å§‹ LoRA è®­ç»ƒ"
fi

echo "ðŸ“¥ åŠ è½½ Base Model: ${BASE_MODEL_PATH}"

# deepspeed --include="localhost:0,1,2,3,4,5,6,7" main_sft.py \
deepspeed --exclude="localhost:0" main_sft.py \
    --deepspeed="./configs/zero2.json" \
    --pretrained_model_name_or_path=$BASE_MODEL_PATH \
    --pretrained_text_encoder_name_or_path=$TEXT_ENCODER_NAME \
    --pretrained_vision_encoder_name_or_path=$VISION_ENCODER_NAME \
    --output_dir=$LORA_OUTPUT_DIR \
    --seed=${seed} \
    --use_lora \
    --lora_rank=${lora_rank} \
    --lora_alpha=${lora_alpha} \
    --lora_dropout=0.1 \
    --lora_target_modules="all" \
    --train_batch_size=48 \
    --gradient_accumulation_steps=4 \
    --sample_batch_size=32 \
    --num_sample_batches=4 \
    --max_train_steps=200000 \
    --checkpointing_period=2000 \
    --sample_period=500 \
    --checkpoints_total_limit=40 \
    --lr_scheduler="constant_with_warmup" \
    --learning_rate=1e-4 \
    --lr_warmup_steps=6000 \
    --mixed_precision="bf16" \
    --dataloader_num_workers=8 \
    --image_aug \
    --dataset_type="finetune" \
    --state_noise_snr=40 \
    --load_from_hdf5 \
    --report_to=tensorboard \
    --precomp_lang_embed \
    $RESUME_ARG

