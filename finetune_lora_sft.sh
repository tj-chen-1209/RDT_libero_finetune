export run_id=$(date +%Y%m%d_%H%M%S)
export NCCL_IB_HCA=mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1,mlx5_7:1,mlx5_8:1,mlx5_9:1
export NCCL_IB_DISABLE=0
export NCCL_SOCKET_IFNAME=bond0
export NCCL_DEBUG=INFO
export NCCL_NVLS_ENABLE=0
export DS_BUILD_EVOFORMER_ATTN=0
# export CUDA_VISIBLE_DEVICES="1,2,3,4,5,6,7"
# export CUDA_VISIBLE_DEVICES=0


export TEXT_ENCODER_NAME="google/t5-v1_1-xxl"
export VISION_ENCODER_NAME="google/siglip-so400m-patch14-384"
export CFLAGS="-I/usr/include"
export LDFLAGS="-L/usr/lib/x86_64-linux-gnu"

#========================================================================
# è®­ç»ƒé…ç½®ï¼šæ•°æ®é›†å’Œå¾®è°ƒæ–¹æ³•
#========================================================================
dataset_name="libero_spatial"      # æ•°æ®é›†: libero_10, libero_spatial, libero_object, libero_goal, libero_90
finetune_method="LoRA"             # å¾®è°ƒæ–¹æ³•: LoRA (å‚æ•°é«˜æ•ˆ) æˆ– Full (å…¨å‚æ•°)
model_size="1B"                    # æ¨¡åž‹å¤§å°: 1B
lora_rank=32                       # LoRA rank
lora_alpha=64                      # LoRA alpha
seed=42                            # éšæœºç§å­ï¼ˆç”¨äºŽå¯å¤çŽ°æ€§ï¼‰

# åŸºç¡€æ¨¡åž‹è·¯å¾„ï¼ˆå¦‚æžœä»Žcheckpointç»§ç»­è®­ç»ƒï¼‰
base_model_name="spatial-ckpt20k"  # åŸºç¡€æ¨¡åž‹æ ‡è¯†ï¼šscratch(ä»Žå¤´), spatial-ckpt20k(ä»Žspatial 20kæ­¥ç»§ç»­)
BASE_MODEL_PATH="./checkpoints/rdt-rdt_libero_sft_csq-libero_spatial-20251125_114702/checkpoint-20000"

export WANDB_PROJECT="rdt_libero_sft_lora_csq"

#========================================================================
# LoRA å¾®è°ƒæ¨¡å¼ï¼ˆæŽ¨èç”¨äºŽå¿«é€Ÿå®žéªŒå’Œèµ„æºå—é™åœºæ™¯ï¼‰
# ========================================================================
# ä¼˜åŠ¿ï¼š
#   - æ˜¾å­˜å ç”¨å°‘ï¼ˆçº¦èŠ‚çœ50%ï¼‰
#   - è®­ç»ƒé€Ÿåº¦å¿«ï¼ˆå¿«1.5-2å€ï¼‰
#   - æƒé‡æ–‡ä»¶å°ï¼ˆå‡ MB vs å‡ GBï¼‰
#   - ä¾¿äºŽç‰ˆæœ¬ç®¡ç†å’Œåˆ†äº«
#========================================================================

# ç”Ÿæˆæ¸…æ™°çš„è¾“å‡ºæ–‡ä»¶å¤¹åç§°
# æ ¼å¼: RDT-{model_size}-{method}-{dataset}-from_{base_model}-r{rank}a{alpha}-{timestamp}
if [ -z "$BASE_MODEL_PATH" ]; then
    # ä»Žå¤´è®­ç»ƒ
    export LORA_OUTPUT_DIR="./checkpoints/RDT-${model_size}-${finetune_method}-${dataset_name}-r${lora_rank}a${lora_alpha}-${run_id}"
else
    # ä»Žcheckpointç»§ç»­è®­ç»ƒ
    export LORA_OUTPUT_DIR="./checkpoints/RDT-${model_size}-${finetune_method}-${dataset_name}-from_${base_model_name}-r${lora_rank}a${lora_alpha}-${run_id}"
fi

#========================================================================
# æ‰“å°è®­ç»ƒé…ç½®
#========================================================================
echo "=========================================================================="
echo "ðŸš€ RDT LoRA å¾®è°ƒè®­ç»ƒ"
echo "=========================================================================="
echo "ðŸ“Š æ•°æ®é›†:        ${dataset_name}"
echo "ðŸ”§ å¾®è°ƒæ–¹æ³•:      ${finetune_method}"
echo "ðŸ“¦ æ¨¡åž‹å¤§å°:      ${model_size}"
if [ -n "$BASE_MODEL_PATH" ]; then
    echo "ðŸŽ¯ åŸºç¡€æ¨¡åž‹:      ${base_model_name}"
    echo "ðŸ“‚ æ¨¡åž‹è·¯å¾„:      ${BASE_MODEL_PATH}"
else
    echo "ðŸŽ¯ åŸºç¡€æ¨¡åž‹:      ä»Žå¤´è®­ç»ƒ (scratch)"
fi
echo "ðŸŽ¯ LoRA Rank:     ${lora_rank}"
echo "ðŸŽ¯ LoRA Alpha:    ${lora_alpha}"
echo "ðŸŒ± éšæœºç§å­:      ${seed}"
echo "ðŸ“ è¾“å‡ºç›®å½•:      ${LORA_OUTPUT_DIR}"
echo "â° è¿è¡Œæ—¶é—´æˆ³:    ${run_id}"
echo "=========================================================================="

if [ ! -d "$LORA_OUTPUT_DIR" ]; then
    mkdir -p "$LORA_OUTPUT_DIR"
    echo "âœ… è¾“å‡ºæ–‡ä»¶å¤¹å·²åˆ›å»º: '$LORA_OUTPUT_DIR'"
else
    echo "âš ï¸  è¾“å‡ºæ–‡ä»¶å¤¹å·²å­˜åœ¨: '$LORA_OUTPUT_DIR'"
fi

# ä¿å­˜è®­ç»ƒé…ç½®åˆ°è¾“å‡ºç›®å½•
cat > "$LORA_OUTPUT_DIR/training_config.txt" << EOF
è®­ç»ƒé…ç½®ä¿¡æ¯
=====================================
æ•°æ®é›†:          ${dataset_name}
å¾®è°ƒæ–¹æ³•:        ${finetune_method}
æ¨¡åž‹å¤§å°:        ${model_size}
åŸºç¡€æ¨¡åž‹:        ${base_model_name:-scratch}
åŸºç¡€æ¨¡åž‹è·¯å¾„:    ${BASE_MODEL_PATH:-N/A}
LoRA Rank:       ${lora_rank}
LoRA Alpha:      ${lora_alpha}
LoRA Dropout:    0.1
éšæœºç§å­:        ${seed}
è®­ç»ƒæ‰¹æ¬¡å¤§å°:    32
å­¦ä¹ çŽ‡:          1e-4
æ··åˆç²¾åº¦:        bf16
æœ€å¤§è®­ç»ƒæ­¥æ•°:    200000
è¿è¡Œæ—¶é—´æˆ³:      ${run_id}
=====================================
EOF

echo ""
echo "ðŸ”„ å¼€å§‹è®­ç»ƒ..."
echo ""

# æž„å»º deepspeed å‘½ä»¤
if [ -n "$BASE_MODEL_PATH" ]; then
    # ä»Žcheckpointç»§ç»­è®­ç»ƒ
    echo "ðŸ“¥ åŠ è½½åŸºç¡€æ¨¡åž‹: ${BASE_MODEL_PATH}"
    deepspeed --exclude="localhost:0" main_sft.py \
        --deepspeed="./configs/zero2.json" \
        --pretrained_model_name_or_path=$BASE_MODEL_PATH \
        --pretrained_text_encoder_name_or_path=$TEXT_ENCODER_NAME \
        --pretrained_vision_encoder_name_or_path=$VISION_ENCODER_NAME \
        --output_dir=$LORA_OUTPUT_DIR \
        --seed=${seed} \
        --use_lora \
        --lora_rank=${lora_rank} \
        --lora_alpha=${lora_alpha} \
        --lora_dropout=0.1 \
        --lora_target_modules="all" \
        --train_batch_size=32 \
        --sample_batch_size=32 \
        --num_sample_batches=4 \
        --max_train_steps=200000 \
        --checkpointing_period=5000 \
        --sample_period=500 \
        --checkpoints_total_limit=40 \
        --lr_scheduler="constant" \
        --learning_rate=1e-4 \
        --mixed_precision="bf16" \
        --dataloader_num_workers=8 \
        --image_aug \
        --dataset_type="finetune" \
        --state_noise_snr=40 \
        --load_from_hdf5 \
        --report_to=tensorboard \
        --precomp_lang_embed
else
    # ä»Žå¤´è®­ç»ƒ
    echo "ðŸ†• ä»Žå¤´å¼€å§‹è®­ç»ƒ"
    deepspeed --exclude="localhost:0" main_sft.py \
        --deepspeed="./configs/zero2.json" \
        --pretrained_text_encoder_name_or_path=$TEXT_ENCODER_NAME \
        --pretrained_vision_encoder_name_or_path=$VISION_ENCODER_NAME \
        --output_dir=$LORA_OUTPUT_DIR \
        --seed=${seed} \
        --use_lora \
        --lora_rank=${lora_rank} \
        --lora_alpha=${lora_alpha} \
        --lora_dropout=0.1 \
        --lora_target_modules="all" \
        --train_batch_size=32 \
        --sample_batch_size=32 \
        --num_sample_batches=4 \
        --max_train_steps=200000 \
        --checkpointing_period=5000 \
        --sample_period=500 \
        --checkpoints_total_limit=40 \
        --lr_scheduler="constant" \
        --learning_rate=1e-4 \
        --mixed_precision="bf16" \
        --dataloader_num_workers=8 \
        --image_aug \
        --dataset_type="finetune" \
        --state_noise_snr=40 \
        --load_from_hdf5 \
        --report_to=tensorboard \
        --precomp_lang_embed
fi

