# RDT-1B ç®—æ³•æµç¨‹è¯¦è§£ï¼ˆå…·ä½“å®ç°ç‰ˆï¼‰

æœ¬æ–‡æ¡£ä»ç®—æ³•å¤´åˆ°å°¾ï¼Œé€ä¸€æ¨¡å—è§£é‡Š RDTï¼ˆRobotics Diffusion Transformerï¼‰çš„å®Œæ•´å·¥ä½œæµç¨‹ï¼Œ**åŒ…å«æ¯ä¸ªæ­¥éª¤çš„å…·ä½“ä»£ç å®ç°ã€æ•°æ®å½¢çŠ¶å˜åŒ–å’Œå‡½æ•°è°ƒç”¨é“¾**ã€‚

---

## ğŸ“‹ ç›®å½•

1. [æ•´ä½“æ¶æ„æ¦‚è§ˆ](#1-æ•´ä½“æ¶æ„æ¦‚è§ˆ)
2. [æ•°æ®åŠ è½½æ¨¡å—ï¼ˆå…·ä½“å®ç°ï¼‰](#2-æ•°æ®åŠ è½½æ¨¡å—å…·ä½“å®ç°)
3. [å¤šæ¨¡æ€ç¼–ç æ¨¡å—ï¼ˆå…·ä½“å®ç°ï¼‰](#3-å¤šæ¨¡æ€ç¼–ç æ¨¡å—å…·ä½“å®ç°)
4. [æ‰©æ•£æ¨¡å‹æ ¸å¿ƒï¼ˆå…·ä½“å®ç°ï¼‰](#4-æ‰©æ•£æ¨¡å‹æ ¸å¿ƒå…·ä½“å®ç°)
5. [è®­ç»ƒæµç¨‹ï¼ˆå®Œæ•´ä»£ç ï¼‰](#5-è®­ç»ƒæµç¨‹å®Œæ•´ä»£ç )
6. [æ¨ç†æµç¨‹ï¼ˆå®Œæ•´ä»£ç ï¼‰](#6-æ¨ç†æµç¨‹å®Œæ•´ä»£ç )
7. [å…³é”®å‡½æ•°è°ƒç”¨é“¾](#7-å…³é”®å‡½æ•°è°ƒç”¨é“¾)

---

## 1. æ•´ä½“æ¶æ„æ¦‚è§ˆ

### 1.1 æ ¸å¿ƒæ€æƒ³

RDT å°†**æœºå™¨äººåŠ¨ä½œé¢„æµ‹é—®é¢˜è½¬åŒ–ä¸ºæ‰©æ•£å»å™ªé—®é¢˜**ï¼š

- **è¾“å…¥**ï¼šè¯­è¨€æŒ‡ä»¤ + å¤šè§†è§’å›¾åƒ + å½“å‰çŠ¶æ€
- **è¾“å‡º**ï¼šæœªæ¥64æ­¥åŠ¨ä½œåºåˆ—ï¼ˆ128ç»´ç»Ÿä¸€åŠ¨ä½œå‘é‡ï¼‰
- **æ–¹æ³•**ï¼šæ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelï¼‰+ Transformeræ¶æ„

### 1.2 ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è¾“å…¥å±‚                                  â”‚
â”‚  è¯­è¨€æŒ‡ä»¤(T5)  +  å›¾åƒ(SigLIP)  +  çŠ¶æ€(128ç»´)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 æ¡ä»¶é€‚é…å™¨å±‚                              â”‚
â”‚  è¯­è¨€é€‚é…å™¨  +  å›¾åƒé€‚é…å™¨  +  çŠ¶æ€é€‚é…å™¨                â”‚
â”‚   (4096â†’2048)   (1152â†’2048)   (256â†’2048)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                RDT Transformer                          â”‚
â”‚  [æ—¶é—´æ­¥åµŒå…¥] + [é¢‘ç‡åµŒå…¥] + [çŠ¶æ€] + [åŠ¨ä½œtokens]     â”‚
â”‚         â†“                                               â”‚
â”‚  28å±‚ RDT Block (äº¤æ›¿æ³¨å…¥è¯­è¨€/å›¾åƒæ¡ä»¶)                 â”‚
â”‚         â†“                                               â”‚
â”‚  è¾“å‡ºå±‚ (2048 â†’ 128ç»´åŠ¨ä½œ)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              æ‰©æ•£å»å™ªè¿‡ç¨‹ (æ¨ç†æ—¶)                        â”‚
â”‚  çº¯å™ªå£° â†’ DPM-Solver(5æ­¥) â†’ åŠ¨ä½œåºåˆ—                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. æ•°æ®åŠ è½½æ¨¡å—ï¼ˆå…·ä½“å®ç°ï¼‰

### 2.1 ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼

**è®¾è®¡ç›®çš„**ï¼šé«˜æ•ˆå¤„ç†å¤§è§„æ¨¡æ•°æ®ï¼ˆ1M+ episodesï¼‰

**å…·ä½“å®ç°ä½ç½®**ï¼š`train/dataset.py` - `VLAConsumerDataset` ç±»

**æ•°æ®æµå‘**ï¼š
```
Producer (åå°è¿›ç¨‹)          Consumer (è®­ç»ƒè¿›ç¨‹)
    â†“                           â†“
åŸå§‹æ•°æ®é›†               â†’    ç¼“å†²åŒº     â†  è¯»å–æ•°æ®
    â†“                           â†“
é¢„å¤„ç†æ•°æ®                    æ ‡è®°dirty
    â†“                           â†“
å†™å…¥ç¼“å†²åŒº                    ç»§ç»­è®­ç»ƒ
```

**å…³é”®æœºåˆ¶**ï¼š
- **ç¼“å†²åŒº**ï¼šåˆ†ä¸º512ä¸ªchunkï¼Œæ¯ä¸ªchunkåŒ…å«512ä¸ªæ ·æœ¬
- **Dirty Bit**ï¼šæ ‡è®°å·²è¯»æ ·æœ¬ï¼ŒProducerä¼šæ›¿æ¢è¿™äº›æ ·æœ¬
- **æ–‡ä»¶é”**ï¼šä¿è¯å¹¶å‘å®‰å…¨

### 2.1.1 æ•°æ®åŠ è½½å…·ä½“å®ç°

**å‡½æ•°è°ƒç”¨é“¾**ï¼š
```python
# train/train.py: 271-278
train_dataloader = torch.utils.data.DataLoader(
    train_dataset,  # VLAConsumerDatasetå®ä¾‹
    batch_size=args.train_batch_size,  # é»˜è®¤32
    shuffle=True,
    collate_fn=data_collator,  # DataCollatorForVLAConsumerDataset
    num_workers=args.dataloader_num_workers,  # é»˜è®¤8
    pin_memory=True,
    persistent_workers=True
)
```

**å•ä¸ªæ ·æœ¬åŠ è½½æµç¨‹**ï¼ˆ`VLAConsumerDataset.__getitem__`ï¼‰ï¼š

```python
# train/dataset.py: 268-379
def __getitem__(self, index):
    # æ­¥éª¤1ï¼šä»ç¼“å†²åŒºåŠ è½½åŸå§‹æ•°æ®
    if self.use_hdf5:
        res = self.hdf5_dataset.get_item()  # ç›´æ¥ä»HDF5è¯»å–
    else:
        # ä»ç¼“å†²åŒºè¯»å–ï¼ˆä½¿ç”¨æ–‡ä»¶é”ï¼‰
        (content, _, states, _, actions, _, 
         state_elem_mask, *image_metas, 
         state_std, state_mean, state_norm) = self._safe_load(index)
    
    # æ•°æ®å½¢çŠ¶ï¼š
    # states: (T, 128) - å¯èƒ½å¤šä¸ªæ—¶é—´æ­¥çš„çŠ¶æ€
    # actions: (64, 128) - æœªæ¥64æ­¥åŠ¨ä½œ
    # state_elem_mask: (128,) - åŠ¨ä½œæœ‰æ•ˆç»´åº¦mask
    # image_metas: [cam_high(2, H, W, 3), mask(2,), ...] - å›¾åƒå’Œmaskå¯¹
    
    # æ­¥éª¤2ï¼šåº”ç”¨æ¡ä»¶æ©ç ï¼ˆ10%æ¦‚ç‡ï¼‰
    # çŠ¶æ€æ©ç 
    if random.random() > self.cond_mask_prob:  # 90%ä¿ç•™åŸçŠ¶æ€
        data_dict["states"] = states
    else:  # 10%ç”¨æ•°æ®é›†å‡å€¼æ›¿æ¢
        ds_state_mean = np.array(self.dataset_stat[dataset_name]['state_mean'])
        data_dict["states"] = ds_state_mean
    
    # æ§åˆ¶é¢‘ç‡æ©ç 
    data_dict['ctrl_freq'] = self.control_freq[dataset_name] \
        if random.random() > self.cond_mask_prob else 0
    
    # æ­¥éª¤3ï¼šå›¾åƒé¢„å¤„ç†
    for i in range(self.img_history_size):  # 2ä¸ªå†å²æ—¶é—´æ­¥
        for j in range(self.num_cameras):  # 3ä¸ªç›¸æœº
            image, valid = images[i], image_mask[i]
            if valid and random.random() > mask_probs[j]:
                # 90%ä¿ç•™åŸå›¾åƒ
                rearranged_images.append((image, True))
            else:
                # 10%ç”¨èƒŒæ™¯å›¾åƒæ›¿æ¢
                background_image = np.ones((H, W, 3)) * background_color
                rearranged_images.append((background_image, False))
    
    # æ­¥éª¤4ï¼šå›¾åƒå¢å¼ºï¼ˆ50%æ¦‚ç‡ï¼Œå¦‚æœå¯ç”¨ï¼‰
    if self.image_aug and random.random() > 0.5:
        aug_type = random.choice(["corrupt_only", "color_only", "both"])
        if aug_type != "corrupt_only":
            image = transforms.ColorJitter(
                brightness=0.3, contrast=0.4, saturation=0.5, hue=0.03)(image)
        if aug_type != "color_only":
            image = image_corrupt(image)  # æ·»åŠ å™ªå£°ã€æ¨¡ç³Šç­‰
    
    # æ­¥éª¤5ï¼šå›¾åƒé¢„å¤„ç†ï¼ˆSigLIPæ ¼å¼ï¼‰
    image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]
    # è¾“å‡ºå½¢çŠ¶: (3, 336, 336) æˆ–è‡ªå®šä¹‰å°ºå¯¸
    
    # æ­¥éª¤6ï¼šè¯­è¨€å¤„ç†
    if self.use_precomp_lang_embed:
        # ä½¿ç”¨é¢„è®¡ç®—åµŒå…¥
        data_dict["lang_embed"] = torch.load(content["instruction"])
    else:
        # ä½¿ç”¨T5 tokenizer
        instruction = content["instruction"] \
            if random.random() > self.cond_mask_prob else ""  # 10%æ©ç ä¸ºç©ºå­—ç¬¦ä¸²
        data_dict["input_ids"] = self.tokenizer(
            instruction,
            return_tensors="pt",
            padding="longest",
            truncation=False,
        ).input_ids[0]  # å½¢çŠ¶: (L,) L <= 1024
    
    return data_dict
```

**æ‰¹æ¬¡ç»„è£…**ï¼ˆ`DataCollatorForVLAConsumerDataset`ï¼‰ï¼š

```python
# train/dataset.py: 391-464
def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:
    batch = {
        "states": [],      # åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´  (T, 128)
        "actions": [],     # åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´  (64, 128)
        "images": [],      # åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´  (6, 3, H, W) - 2å†å²Ã—3ç›¸æœº
        "input_ids": [],   # åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´  (L,) - å˜é•¿
        ...
    }
    
    # ç»„è£…æ‰¹æ¬¡
    for instance in instances:
        batch["states"].append(instance["states"])
        batch["actions"].append(instance["actions"])
        batch["images"].append(torch.stack(instance["images"], dim=0))
        batch["input_ids"].append(instance["input_ids"])
    
    # å †å ä¸ºå¼ é‡
    batch["states"] = torch.stack(batch["states"], dim=0)      # (B, T, 128)
    batch["actions"] = torch.stack(batch["actions"], dim=0)     # (B, 64, 128)
    batch["images"] = torch.stack(batch["images"], dim=0)      # (B, 6, 3, H, W)
    
    # å¡«å……å˜é•¿åºåˆ—ï¼ˆè¯­è¨€ï¼‰
    batch["input_ids"] = torch.nn.utils.rnn.pad_sequence(
        batch["input_ids"],
        batch_first=True,
        padding_value=self.tokenizer.pad_token_id
    )  # (B, max_L) max_L <= 1024
    
    return batch
```

### 2.2 æ•°æ®å¢å¼ºç­–ç•¥ï¼ˆå…·ä½“å®ç°ï¼‰

**æ¡ä»¶æ©ç è®­ç»ƒ**ï¼ˆæå‡é²æ£’æ€§ï¼‰ï¼š

1. **çŠ¶æ€æ©ç **ï¼ˆæ¦‚ç‡10%ï¼‰
   ```python
   # train/dataset.py: 309-314
   ds_state_mean = np.array(self.dataset_stat[dataset_name]['state_mean'])  # (128,)
   ds_state_mean = np.tile(ds_state_mean[None], (states.shape[0], 1))  # (T, 128)
   
   if random.random() > self.cond_mask_prob:  # 90%
       data_dict["states"] = states  # ä¿ç•™åŸçŠ¶æ€
   else:  # 10%
       data_dict["states"] = ds_state_mean  # ç”¨å‡å€¼æ›¿æ¢
   ```

2. **å›¾åƒæ©ç **ï¼ˆæ¦‚ç‡10%ï¼‰
   ```python
   # train/dataset.py: 326-339
   background_color = np.array([
       int(x*255) for x in self.image_processor.image_mean
   ], dtype=np.uint8).reshape(1, 1, 3)  # RGBå‡å€¼
   background_image = np.ones((H, W, 3), dtype=np.uint8) * background_color
   
   mask_probs = [self.cond_mask_prob] * self.num_cameras  # [0.1, 0.1, 0.1]
   if self.cam_ext_mask_prob >= 0.0:
       mask_probs[0] = self.cam_ext_mask_prob  # å¤–éƒ¨ç›¸æœºå¯å•ç‹¬è®¾ç½®
   
   if valid and random.random() > mask_probs[j]:  # 90%
       rearranged_images.append((image, True))
   else:  # 10%
       rearranged_images.append((background_image.copy(), False))
   ```

3. **è¯­è¨€æ©ç **ï¼ˆæ¦‚ç‡10%ï¼‰
   ```python
   # train/dataset.py: 359-366
   if self.use_precomp_lang_embed:
       data_dict["lang_embed"] = torch.load(content["instruction"]) \
           if random.random() > self.cond_mask_prob \
           else self.empty_lang_embed  # 10%ç”¨ç©ºåµŒå…¥
   else:
       instruction = content["instruction"] \
           if random.random() > self.cond_mask_prob \
           else ""  # 10%ç”¨ç©ºå­—ç¬¦ä¸²
       data_dict["input_ids"] = self.tokenizer(instruction, ...)
   ```

4. **æ§åˆ¶é¢‘ç‡æ©ç **ï¼ˆæ¦‚ç‡10%ï¼‰
   ```python
   # train/dataset.py: 299-300
   data_dict['ctrl_freq'] = self.control_freq[dataset_name] \
       if random.random() > self.cond_mask_prob else 0  # 10%è®¾ä¸º0
   ```

5. **çŠ¶æ€å™ªå£°**ï¼ˆå¯é€‰ï¼ŒSNR=40dBï¼‰
   ```python
   # train/dataset.py: 302-307
   if self.state_noise_snr is not None:
       # å™ªå£°æ ‡å‡†å·® = state_std / sqrt(10^(SNR/10))
       noise_std = state_std / np.sqrt(10 ** (self.state_noise_snr / 10))
       states += np.random.normal(0.0, noise_std, states.shape)
   ```

### 2.3 ç»Ÿä¸€åŠ¨ä½œç©ºé—´

**128ç»´ç»Ÿä¸€å‘é‡**ç»“æ„ï¼š

```
[0-10)     å³è‡‚å…³èŠ‚ä½ç½®
[10-15)    å³è‡‚å¤¹çˆªä½ç½®
[15-25)    å³è‡‚å…³èŠ‚é€Ÿåº¦
[30-39)    å³è‡‚æœ«ç«¯ä½ç½®å’Œ6Då§¿æ€
[50-60)    å·¦è‡‚å…³èŠ‚ä½ç½®
[60-65)    å·¦è‡‚å¤¹çˆªä½ç½®
[65-75)    å·¦è‡‚å…³èŠ‚é€Ÿåº¦
[80-89)    å·¦è‡‚æœ«ç«¯ä½ç½®å’Œ6Då§¿æ€
[100-103)  åŸºåº§çº¿é€Ÿåº¦å’Œè§’é€Ÿåº¦
```

**ä¼˜åŠ¿**ï¼š
- æ”¯æŒå•è‡‚/åŒè‡‚æœºå™¨äºº
- æ”¯æŒå…³èŠ‚æ§åˆ¶/æœ«ç«¯æ§åˆ¶
- æ˜“äºæ‰©å±•åˆ°ä¸åŒæœºå™¨äººå¹³å°

---

## 3. å¤šæ¨¡æ€ç¼–ç æ¨¡å—ï¼ˆå…·ä½“å®ç°ï¼‰

### 3.1 è¯­è¨€ç¼–ç ï¼ˆT5-XXLï¼‰

**å…·ä½“å®ç°ä½ç½®**ï¼š
- ç¼–ç å™¨ï¼š`models/multimodal_encoder/t5_encoder.py`
- é€‚é…å™¨ï¼š`models/rdt_runner.py` - `build_condition_adapter`

**å®Œæ•´æµç¨‹**ï¼š
```
è‡ªç„¶è¯­è¨€æŒ‡ä»¤: "Pick up the cup"
    â†“
T5 Tokenizer (åˆ†è¯)
    â†“
T5 Encoder (4096ç»´)
    â†“
è¯­è¨€é€‚é…å™¨ (MLP: 4096 â†’ 2048)
    â†“
ç»Ÿä¸€éšè—ç©ºé—´ (2048ç»´)
```

**å…·ä½“ä»£ç å®ç°**ï¼š

```python
# train/train.py: 137-139
text_embedder = T5Embedder(
    from_pretrained=args.pretrained_text_encoder_name_or_path,  # "google/t5-v1_1-xxl"
    model_max_length=config["dataset"]["tokenizer_max_length"],  # 1024
    device=accelerator.device
)
tokenizer, text_encoder = text_embedder.tokenizer, text_embedder.model

# train/train.py: 419-430
with torch.no_grad():  # å†»ç»“ç¼–ç å™¨ï¼Œä¸è®¡ç®—æ¢¯åº¦
    lang_attn_mask = batch["lang_attn_mask"]  # (B, L_lang) bool
    
    if args.precomp_lang_embed:
        # ä½¿ç”¨é¢„è®¡ç®—åµŒå…¥ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
        text_embeds = batch["lang_embeds"].to(dtype=weight_dtype)  # (B, L, 4096)
    else:
        # å®æ—¶ç¼–ç 
        text_embeds = text_encoder(
            input_ids=batch["input_ids"],      # (B, L) L <= 1024
            attention_mask=lang_attn_mask
        )["last_hidden_state"].detach()  # (B, L, 4096)

# æ•°æ®å½¢çŠ¶å˜åŒ–ï¼š
# input_ids: (B, L) int32 â†’ text_embeds: (B, L, 4096) float32
```

**è¯­è¨€é€‚é…å™¨æ„å»º**ï¼š

```python
# models/rdt_runner.py: 107-140
def build_condition_adapter(self, projector_type, in_features, out_features):
    # projector_type = "mlp2x_gelu"
    mlp_gelu_match = re.match(r'^mlp(\d+)x_gelu$', projector_type)
    mlp_depth = int(mlp_gelu_match.group(1))  # 2
    
    modules = [
        nn.Linear(4096, 2048),  # ç¬¬ä¸€å±‚ï¼šé™ç»´
    ]
    for _ in range(1, mlp_depth):  # ç¬¬äºŒå±‚
        modules.append(nn.GELU(approximate="tanh"))
        modules.append(nn.Linear(2048, 2048))  # ä¿æŒç»´åº¦
    
    projector = nn.Sequential(*modules)
    # ç»“æ„: Linear(4096â†’2048) â†’ GELU â†’ Linear(2048â†’2048)

# models/rdt_runner.py: 154-155
adpated_lang = self.lang_adaptor(lang_tokens)
# è¾“å…¥: (B, L, 4096) â†’ è¾“å‡º: (B, L, 2048)
```

**ç‰¹ç‚¹**ï¼š
- ä½¿ç”¨é¢„è®­ç»ƒT5-XXLæ¨¡å‹ï¼ˆå†»ç»“å‚æ•°ï¼‰
- æ”¯æŒå˜é•¿è¯­è¨€æŒ‡ä»¤ï¼ˆæœ€å¤§1024 tokensï¼‰
- å¯é¢„è®¡ç®—è¯­è¨€åµŒå…¥èŠ‚çœæ˜¾å­˜

### 3.2 å›¾åƒç¼–ç ï¼ˆSigLIPï¼‰

**å…·ä½“å®ç°ä½ç½®**ï¼š
- ç¼–ç å™¨ï¼š`models/multimodal_encoder/siglip_encoder.py`
- é€‚é…å™¨ï¼š`models/rdt_runner.py` - `build_condition_adapter`

**å®Œæ•´æµç¨‹**ï¼š
```
å¤šè§†è§’å›¾åƒ (2å†å² Ã— 3ç›¸æœº = 6å¼ å›¾åƒ)
    â†“
SigLIP Encoder (æ¯å¼ å›¾åƒ â†’ 1152ç»´)
    â†“
å›¾åƒé€‚é…å™¨ (MLP: 1152 â†’ 2048)
    â†“
ç»Ÿä¸€éšè—ç©ºé—´ (2048ç»´)
```

**å…·ä½“ä»£ç å®ç°**ï¼š

```python
# train/train.py: 141-142
vision_encoder = SiglipVisionTower(
    vision_tower=args.pretrained_vision_encoder_name_or_path,  # "google/siglip-so400m-patch14-384"
    args=None
)
image_processor = vision_encoder.image_processor

# train/train.py: 417-421
with torch.no_grad():
    batch_size, _, C, H, W = images.shape  # (B, 6, 3, 336, 336)
    
    # å±•å¹³æ‰€æœ‰å›¾åƒè¿›è¡Œæ‰¹é‡ç¼–ç 
    image_embeds = vision_encoder(
        images.reshape(-1, C, H, W)  # (B*6, 3, 336, 336)
    ).detach()  # (B*6, num_patches, 1152)
    
    # é‡å¡‘ä¸ºæ‰¹æ¬¡æ ¼å¼
    image_embeds = image_embeds.reshape(
        (batch_size, -1, vision_encoder.hidden_size)
    )  # (B, 6*num_patches, 1152)

# æ•°æ®å½¢çŠ¶å˜åŒ–ï¼š
# images: (B, 6, 3, 336, 336) 
#   â†’ reshape: (B*6, 3, 336, 336)
#   â†’ SigLIP: (B*6, 529, 1152)  # 529 = (336/14)^2 = 24^2 patches
#   â†’ reshape: (B, 6*529, 1152) = (B, 3174, 1152)
```

**å›¾åƒé€‚é…å™¨**ï¼š

```python
# models/rdt_runner.py: 59-63
self.img_adaptor = self.build_condition_adapter(
    config['img_adaptor'],      # "mlp2x_gelu"
    in_features=1152,           # SigLIPè¾“å‡ºç»´åº¦
    out_features=2048           # RDTéšè—ç©ºé—´
)

# models/rdt_runner.py: 165-166
adpated_img = self.img_adaptor(img_tokens)
# è¾“å…¥: (B, 3174, 1152) â†’ è¾“å‡º: (B, 3174, 2048)
```

**ç‰¹ç‚¹**ï¼š
- ä½¿ç”¨é¢„è®­ç»ƒSigLIPæ¨¡å‹ï¼ˆå†»ç»“å‚æ•°ï¼‰
- æ”¯æŒå¤šè§†è§’ï¼ˆå¤–éƒ¨ç›¸æœº + å·¦å³æ‰‹è…•ç›¸æœºï¼‰
- æ”¯æŒå†å²å›¾åƒï¼ˆimg_history_size=2ï¼‰
- æ¯å¼ å›¾åƒè¢«åˆ†æˆ529ä¸ªpatchesï¼ˆ24Ã—24ï¼‰ï¼Œæ¯ä¸ªpatchç¼–ç ä¸º1152ç»´

### 3.3 çŠ¶æ€ç¼–ç 

**å…·ä½“å®ç°ä½ç½®**ï¼š`models/rdt_runner.py` - `state_adaptor`

**å®Œæ•´æµç¨‹**ï¼š
```
çŠ¶æ€å‘é‡ (128ç»´) + Mask (128ç»´) = 256ç»´
    â†“
çŠ¶æ€é€‚é…å™¨ (MLP: 256 â†’ 2048)
    â†“
ç»Ÿä¸€éšè—ç©ºé—´ (2048ç»´)
```

**å…·ä½“ä»£ç å®ç°**ï¼š

```python
# train/train.py: 407-410
states = batch["states"].to(dtype=weight_dtype)  # (B, T, 128)
states = states[:, -1:, :]  # åªä½¿ç”¨æœ€åä¸€ä¸ªçŠ¶æ€: (B, 1, 128)
state_elem_mask = batch["state_elem_mask"].to(dtype=weight_dtype)  # (B, 1, 128)

# models/rdt_runner.py: 52-56
self.state_adaptor = self.build_condition_adapter(
    config['state_adaptor'],     # "mlp3x_gelu" (3å±‚MLP)
    in_features=state_token_dim * 2,  # 128 * 2 = 256
    out_features=2048
)

# è®­ç»ƒæ—¶ï¼šæ‹¼æ¥çŠ¶æ€ã€åŠ¨ä½œå’Œmask
# models/rdt_runner.py: 284-290
state_action_traj = torch.cat([state_tokens, noisy_action], dim=1)  
# (B, 1+64, 128)

action_mask = action_mask.expand(-1, state_action_traj.shape[1], -1)
# (B, 65, 128)

state_action_traj = torch.cat([state_action_traj, action_mask], dim=2)
# (B, 65, 256) = [çŠ¶æ€(128) + åŠ¨ä½œ(128), mask(128)]

# models/rdt_runner.py: 167-168
adpated_state = self.state_adaptor(state_tokens)
# è¾“å…¥: (B, 65, 256) â†’ è¾“å‡º: (B, 65, 2048)
```

**ç‰¹ç‚¹**ï¼š
- Maskè¡¨ç¤ºæœ‰æ•ˆç»´åº¦ï¼ˆ0-1æµ®ç‚¹å¼ é‡ï¼‰
- æ”¯æŒä¸åŒæœºå™¨äººçš„ä¸åŒåŠ¨ä½œç©ºé—´
- ä½¿ç”¨3å±‚MLPï¼ˆæ¯”è¯­è¨€/å›¾åƒé€‚é…å™¨æ›´æ·±ï¼‰

---

## 4. æ‰©æ•£æ¨¡å‹æ ¸å¿ƒï¼ˆå…·ä½“å®ç°ï¼‰

### 4.1 RDT Transformeræ¶æ„

**å…·ä½“å®ç°ä½ç½®**ï¼š`models/rdt/model.py` - `RDT` ç±»

**è¾“å…¥åºåˆ—æ„å»º**ï¼š

```python
# models/rdt/model.py: 187-197
# æ­¥éª¤1ï¼šåµŒå…¥æ—¶é—´æ­¥å’Œæ§åˆ¶é¢‘ç‡
# æ—¶é—´æ­¥åµŒå…¥è¯¦ç»†è¿‡ç¨‹ï¼š
# models/rdt/blocks.py: 65-68
t = self.t_embedder(t)  # t: (B,) æˆ– (1,) æ ‡é‡æ—¶é—´æ­¥
# å†…éƒ¨è¿‡ç¨‹:
# 1. timestep_embedding(t, dim=256):
#    - è®¡ç®—é¢‘ç‡: omega[i] = 1 / (10000^(i/128))
#    - ç›¸ä½: phase = t * omega
#    - ç¼–ç : [cos(phase), sin(phase)] â†’ (B, 256)
# 2. MLPæ˜ å°„: Linear(256â†’2048) â†’ SiLU â†’ Linear(2048â†’2048)
# è¾“å‡º: (B, 2048)
t = t.unsqueeze(1)  # (B, 1, 2048)

# æ§åˆ¶é¢‘ç‡åµŒå…¥ï¼ˆç±»ä¼¼æ—¶é—´æ­¥ï¼‰
freq = self.freq_embedder(freq)  # freq: (B,) æ ‡é‡é¢‘ç‡
# ä¾‹å¦‚: 10.0 (10Hz), 30.0 (30Hz), 0.0 (æ©ç )
freq = freq.unsqueeze(1)  # (B, 1, 2048)

# æ­¥éª¤2ï¼šæ‹¼æ¥è¾“å…¥åºåˆ—
x = torch.cat([t, freq, x], dim=1)  
# è¾“å…¥ x: (B, 65, 2048) = [çŠ¶æ€(1) + åŠ¨ä½œ(64)]
# è¾“å‡º: (B, 1+1+65, 2048) = (B, 67, 2048)
# åºåˆ—ç»“æ„ï¼š[æ—¶é—´æ­¥(1) + é¢‘ç‡(1) + çŠ¶æ€(1) + åŠ¨ä½œ(64)]

# æ­¥éª¤3ï¼šæ·»åŠ ä½ç½®ç¼–ç 
x = x + self.x_pos_embed  # (B, 67, 2048)
# x_pos_embed: (1, 67, 2048) å¯è®­ç»ƒå‚æ•°ï¼Œä½¿ç”¨å¤šæ¨¡æ€æ­£å¼¦ä½™å¼¦åˆå§‹åŒ–
# ä½ç½®ç¼–ç è€ƒè™‘æ¨¡æ€ä¿¡æ¯:
# - å‰åŠéƒ¨åˆ†: æ¨¡æ€åµŒå…¥ï¼ˆæ—¶é—´æ­¥/é¢‘ç‡/çŠ¶æ€/åŠ¨ä½œæ˜¯ä¸åŒçš„æ¨¡æ€ï¼‰
# - ååŠéƒ¨åˆ†: ä½ç½®åµŒå…¥ï¼ˆåŒä¸€æ¨¡æ€å†…çš„ä½ç½®ï¼‰
```

**ä½ç½®ç¼–ç åˆå§‹åŒ–**ï¼š

```python
# models/rdt/model.py: 105-114
x_pos_embed = get_multimodal_cond_pos_embed(
    embed_dim=2048,
    mm_cond_lens=OrderedDict([
        ('timestep', 1),      # æ—¶é—´æ­¥token
        ('ctrl_freq', 1),     # é¢‘ç‡token
        ('state', 1),         # çŠ¶æ€token
        ('action', 64),       # 64ä¸ªåŠ¨ä½œtokens
    ])
)  # è¾“å‡º: (67, 2048)
self.x_pos_embed.data.copy_(torch.from_numpy(x_pos_embed).float().unsqueeze(0))
```

**RDT Blockç»“æ„**ï¼ˆ28å±‚ï¼Œå…·ä½“å®ç°ï¼‰ï¼š

```python
# models/rdt/blocks.py: 255-285
def forward(self, x, c, mask=None):
    # è¾“å…¥:
    # x: (B, 67, 2048) - ä¸»åºåˆ— [æ—¶é—´æ­¥+é¢‘ç‡+çŠ¶æ€+åŠ¨ä½œ]
    # c: (B, L, 2048) - æ¡ä»¶ï¼ˆè¯­è¨€æˆ–å›¾åƒï¼‰
    #   - è¯­è¨€æ¡ä»¶: (B, L_lang, 2048) L_lang <= 1024
    #   - å›¾åƒæ¡ä»¶: (B, 3174, 2048)
    
    # ========== å­å—1ï¼šè‡ªæ³¨æ„åŠ› + æ®‹å·® ==========
    origin_x = x  # (B, 67, 2048)
    x = self.norm1(x)  # RMSNorm: (B, 67, 2048)
    x = self.attn(x)  # è‡ªæ³¨æ„åŠ›: (B, 67, 2048)
    # è‡ªæ³¨æ„åŠ›è®¡ç®—ï¼ˆtimmåº“å®ç°ï¼‰:
    # Q = K = V = x (B, 67, 2048)
    # Q, K, V â†’ Linear â†’ (B, 67, 2048*3) â†’ split
    # Q, K, V: (B, 67, 32, 64)  # 32å¤´ï¼Œæ¯å¤´64ç»´
    # Attention = softmax(Q @ K^T / sqrt(64)) @ V
    # è¾“å‡º: (B, 67, 2048)
    x = x + origin_x  # æ®‹å·®è¿æ¥
    
    # ========== å­å—2ï¼šäº¤å‰æ³¨æ„åŠ› + æ®‹å·®ï¼ˆæ³¨å…¥æ¡ä»¶ï¼‰ ==========
    origin_x = x  # (B, 67, 2048)
    x = self.norm2(x)  # RMSNorm
    
    # äº¤å‰æ³¨æ„åŠ›è¯¦ç»†è®¡ç®—è¿‡ç¨‹
    # models/rdt/blocks.py: 148-214
    x = self.cross_attn(x, c, mask)
    # å†…éƒ¨è¿‡ç¨‹ï¼š
    # 1. ç”ŸæˆQueryï¼ˆä»ä¸»åºåˆ—xï¼‰
    #    q = self.q(x)  # Linear(2048â†’2048)
    #    q = q.reshape(B, 67, 32, 64).permute(0,2,1,3)  # (B, 32, 67, 64)
    #    
    # 2. ç”ŸæˆKeyå’ŒValueï¼ˆä»æ¡ä»¶cï¼‰
    #    kv = self.kv(c)  # Linear(2048â†’2048*2)
    #    kv = kv.reshape(B, L, 2, 32, 64).permute(2,0,3,1,4)  # (2, B, 32, L, 64)
    #    k, v = kv.unbind(0)  # å„ (B, 32, L, 64)
    #    
    # 3. å½’ä¸€åŒ–
    #    q, k = self.q_norm(q), self.k_norm(k)
    #    
    # 4. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    #    attn = q @ k.transpose(-2, -1)  # (B, 32, 67, L)
    #    attn = attn / sqrt(64)  # ç¼©æ”¾
    #    
    # 5. åº”ç”¨maskï¼ˆå¦‚æœæœ‰ï¼‰
    #    if mask is not None:
    #        attn = attn.masked_fill_(~mask, float('-inf'))
    #    
    # 6. Softmaxå½’ä¸€åŒ–
    #    attn = attn.softmax(dim=-1)  # (B, 32, 67, L)
    #    
    # 7. åŠ æƒæ±‚å’Œ
    #    x = attn @ v  # (B, 32, 67, 64)
    #    x = x.permute(0,2,1,3).reshape(B, 67, 2048)  # (B, 67, 2048)
    #    
    # 8. è¾“å‡ºæŠ•å½±
    #    x = self.proj(x)  # Linear(2048â†’2048)
    # è¾“å‡º: (B, 67, 2048)
    
    x = x + origin_x  # æ®‹å·®è¿æ¥
    
    # ========== å­å—3ï¼šFFN + æ®‹å·® ==========
    origin_x = x  # (B, 67, 2048)
    x = self.norm3(x)  # RMSNorm
    
    # FFNè®¡ç®—ï¼ˆtimmåº“å®ç°ï¼‰
    x = self.ffn(x)  # MLP: 2048 â†’ 2048 â†’ 2048
    # å†…éƒ¨è¿‡ç¨‹ï¼š
    # x â†’ Linear(2048â†’2048) â†’ GELU â†’ Linear(2048â†’2048)
    # è¾“å‡º: (B, 67, 2048)
    
    x = x + origin_x  # æ®‹å·®è¿æ¥
    
    return x  # (B, 67, 2048)
```

**æ•°å­¦å…¬å¼**ï¼š

**è‡ªæ³¨æ„åŠ›**ï¼š
```
Q = K = V = RMSNorm(x)
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
è¾“å‡º = Attention + xï¼ˆæ®‹å·®è¿æ¥ï¼‰
```

**äº¤å‰æ³¨æ„åŠ›**ï¼š
```
Q = RMSNorm(x)  # ä¸»åºåˆ—
K, V = RMSNorm(c)  # æ¡ä»¶åºåˆ—
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
è¾“å‡º = Attention + xï¼ˆæ®‹å·®è¿æ¥ï¼‰
```

**FFN**ï¼š
```
FFN(x) = Linear_2(GELU(Linear_1(RMSNorm(x))))
è¾“å‡º = FFN(x) + xï¼ˆæ®‹å·®è¿æ¥ï¼‰
```

**äº¤æ›¿æ¡ä»¶æ³¨å…¥**ï¼š

```python
# models/rdt/model.py: 210-218
conds = [lang_c, img_c]  # è¯­è¨€å’Œå›¾åƒæ¡ä»¶
masks = [lang_mask, img_mask]

for i, block in enumerate(self.blocks):  # 28å±‚
    # äº¤æ›¿é€‰æ‹©æ¡ä»¶ï¼ši%2=0ä½¿ç”¨è¯­è¨€ï¼Œi%2=1ä½¿ç”¨å›¾åƒ
    c, mask = conds[i % 2], masks[i % 2]
    # ç¬¬0,2,4,...å±‚ä½¿ç”¨è¯­è¨€æ¡ä»¶
    # ç¬¬1,3,5,...å±‚ä½¿ç”¨å›¾åƒæ¡ä»¶
    x = block(x, c, mask)
```

**æœ€ç»ˆè¾“å‡ºå±‚**ï¼š

```python
# models/rdt/model.py: 220-225
x = self.final_layer(x)  # (B, 67, 128)
# FinalLayer: RMSNorm â†’ MLP(2048 â†’ 2048 â†’ 128)

# åªä¿ç•™åŠ¨ä½œtokensï¼ˆå»æ‰æ—¶é—´æ­¥ã€é¢‘ç‡ã€çŠ¶æ€tokenï¼‰
x = x[:, -64:]  # (B, 64, 128)
return x
```

**å…³é”®è®¾è®¡**ï¼š
- **äº¤æ›¿æ¡ä»¶æ³¨å…¥**ï¼šå¥‡æ•°å±‚ä½¿ç”¨è¯­è¨€æ¡ä»¶ï¼Œå¶æ•°å±‚ä½¿ç”¨å›¾åƒæ¡ä»¶
- **Pre-Normæ¶æ„**ï¼šå½’ä¸€åŒ–åœ¨æ³¨æ„åŠ›/FFNä¹‹å‰ï¼Œè®­ç»ƒæ›´ç¨³å®š
- **RMSå½’ä¸€åŒ–**ï¼šä½¿ç”¨RMSNormè€ŒéLayerNormï¼Œæ›´ç¨³å®š

### 4.2 æ‰©æ•£è®­ç»ƒè¿‡ç¨‹ï¼ˆå…·ä½“å®ç°ï¼‰

**å…·ä½“å®ç°ä½ç½®**ï¼š`models/rdt_runner.py` - `compute_loss`

**å‰å‘æ‰©æ•£ï¼ˆè®­ç»ƒï¼‰å®Œæ•´ä»£ç **ï¼š

```python
# models/rdt_runner.py: 237-315
def compute_loss(self, lang_tokens, lang_attn_mask, img_tokens,
                 state_tokens, action_gt, action_mask, ctrl_freqs):
    batch_size = lang_tokens.shape[0]  # B
    device = lang_tokens.device
    
    # æ­¥éª¤1: é‡‡æ ·å™ªå£° Îµ ~ N(0, I)
    noise = torch.randn(
        action_gt.shape,  # (B, 64, 128)
        dtype=action_gt.dtype,
        device=device
    )
    
    # æ­¥éª¤2: é‡‡æ ·æ—¶é—´æ­¥ t ~ Uniform(0, 1000)
    timesteps = torch.randint(
        0, self.num_train_timesteps,  # [0, 1000)
        (batch_size,), device=device
    ).long()  # (B,) æ¯ä¸ªæ ·æœ¬çš„æ—¶é—´æ­¥å¯ä»¥ä¸åŒ
    
    # æ­¥éª¤3: å‰å‘æ‰©æ•£ï¼šæ·»åŠ å™ªå£°
    # å…¬å¼: x_t = sqrt(alpha_t) * x_0 + sqrt(1 - alpha_t) * epsilon
    noisy_action = self.noise_scheduler.add_noise(
        action_gt,    # (B, 64, 128) - çœŸå®åŠ¨ä½œ x_0
        noise,        # (B, 64, 128) - å™ªå£° Îµ
        timesteps     # (B,) - æ—¶é—´æ­¥ t
    )  # è¾“å‡º: (B, 64, 128) - å¸¦å™ªåŠ¨ä½œ x_t
    
    # æ­¥éª¤4: å‡†å¤‡è¾“å…¥åºåˆ—
    state_action_traj = torch.cat([state_tokens, noisy_action], dim=1)
    # (B, 1+64, 128) = [çŠ¶æ€(1) + åŠ¨ä½œ(64)]
    
    action_mask = action_mask.expand(-1, state_action_traj.shape[1], -1)
    # (B, 65, 128)
    
    state_action_traj = torch.cat([state_action_traj, action_mask], dim=2)
    # (B, 65, 256) = [çŠ¶æ€+åŠ¨ä½œ(128) + mask(128)]
    
    # æ­¥éª¤5: æ¡ä»¶é€‚é…
    lang_cond, img_cond, state_action_traj = self.adapt_conditions(
        lang_tokens, img_tokens, state_action_traj
    )
    # lang_cond: (B, L_lang, 2048)
    # img_cond: (B, 3174, 2048)
    # state_action_traj: (B, 65, 2048)
    
    # æ­¥éª¤6: æ¨¡å‹é¢„æµ‹
    pred = self.model(
        state_action_traj,  # (B, 65, 2048)
        ctrl_freqs,         # (B,)
        timesteps,          # (B,)
        lang_cond,          # (B, L_lang, 2048)
        img_cond,           # (B, 3174, 2048)
        lang_mask=lang_attn_mask
    )  # è¾“å‡º: (B, 64, 128) - é¢„æµ‹çš„åŠ¨ä½œåºåˆ—
    
    # æ­¥éª¤7: ç¡®å®šé¢„æµ‹ç›®æ ‡
    if self.prediction_type == 'sample':
        target = action_gt  # (B, 64, 128) - çœŸå®åŠ¨ä½œ x_0
    elif self.prediction_type == 'epsilon':
        target = noise      # (B, 64, 128) - å™ªå£° Îµ
    
    # æ­¥éª¤8: è®¡ç®—MSEæŸå¤±
    loss = F.mse_loss(pred, target)  # æ ‡é‡
    return loss
```

**å™ªå£°è°ƒåº¦å™¨é…ç½®**ï¼š

```python
# models/rdt_runner.py: 78-83
self.noise_scheduler = DDPMScheduler(
    num_train_timesteps=1000,           # è®­ç»ƒæ—¶é—´æ­¥æ•°
    beta_schedule='squaredcos_cap_v2',  # å™ªå£°è°ƒåº¦ï¼ˆä½™å¼¦å¹³æ–¹ï¼‰
    prediction_type='sample',           # é¢„æµ‹ç›®æ ‡ï¼ˆæ ·æœ¬è€Œéå™ªå£°ï¼‰
    clip_sample=False                   # ä¸è£å‰ªæ ·æœ¬
)

# add_noiseå‡½æ•°å†…éƒ¨å®ç°ï¼ˆdiffusersåº“ï¼‰ï¼š
# alpha_t = 1 - beta_t  (ç´¯ç§¯ä¹˜ç§¯)
# x_t = sqrt(alpha_t) * x_0 + sqrt(1 - alpha_t) * epsilon
```

**ä¸¤ç§é¢„æµ‹ç›®æ ‡**ï¼š
- **'sample'æ¨¡å¼**ï¼šé¢„æµ‹å»å™ªåçš„åŠ¨ä½œ x_0ï¼ˆæœ¬å®ç°ä½¿ç”¨ï¼‰
- **'epsilon'æ¨¡å¼**ï¼šé¢„æµ‹æ·»åŠ çš„å™ªå£° Îµ

### 4.3 æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ï¼ˆå…·ä½“å®ç°ï¼‰

**å…·ä½“å®ç°ä½ç½®**ï¼š`models/rdt_runner.py` - `conditional_sample`

**åå‘å»å™ªï¼ˆæ¨ç†ï¼‰å®Œæ•´ä»£ç **ï¼š

```python
# models/rdt_runner.py: 172-234
def conditional_sample(self, lang_cond, lang_attn_mask, img_cond,
                       state_traj, action_mask, ctrl_freqs):
    device = state_traj.device
    dtype = state_traj.dtype
    
    # æ­¥éª¤1: åˆå§‹åŒ–çº¯å™ªå£° x_T ~ N(0, I)
    noisy_action = torch.randn(
        size=(state_traj.shape[0], self.pred_horizon, self.action_dim),
        # (B, 64, 128)
        dtype=dtype, device=device
    )
    
    # æ‰©å±•åŠ¨ä½œmask
    action_mask = action_mask.expand(-1, self.pred_horizon, -1)
    # (B, 64, 128)
    
    # æ­¥éª¤2: DPM-Solverè®¾ç½®5ä¸ªé‡‡æ ·æ—¶é—´æ­¥
    self.noise_scheduler_sample.set_timesteps(self.num_inference_timesteps)
    # ä»1000ä¸ªè®­ç»ƒæ—¶é—´æ­¥ä¸­é€‰æ‹©5ä¸ªæ—¶é—´æ­¥
    # ä¾‹å¦‚: [999, 799, 599, 399, 199] (ä»å¤§åˆ°å°)
    
    # æ­¥éª¤3: è¿­ä»£å»å™ª (t = T â†’ 0)
    for t in self.noise_scheduler_sample.timesteps:
        # t: æ ‡é‡ï¼Œä¾‹å¦‚ 999, 799, ...
        
        # å‡†å¤‡çŠ¶æ€-åŠ¨ä½œåºåˆ—
        action_traj = torch.cat([noisy_action, action_mask], dim=2)
        # (B, 64, 256) = [åŠ¨ä½œ(128) + mask(128)]
        
        action_traj = self.state_adaptor(action_traj)
        # (B, 64, 2048)
        
        state_action_traj = torch.cat([state_traj, action_traj], dim=1)
        # (B, 1+64, 2048) = [çŠ¶æ€(1) + åŠ¨ä½œ(64)]
        
        # æ¨¡å‹é¢„æµ‹ï¼šé¢„æµ‹å»å™ªåçš„åŠ¨ä½œ
        model_output = self.model(
            state_action_traj,  # (B, 65, 2048)
            ctrl_freqs,         # (B,)
            t.unsqueeze(-1).to(device),  # (1,) å¹¿æ’­åˆ°æ‰¹æ¬¡
            lang_cond,          # (B, L_lang, 2048)
            img_cond,           # (B, 3174, 2048)
            lang_mask=lang_attn_mask
        )  # è¾“å‡º: (B, 64, 128) - é¢„æµ‹çš„åŠ¨ä½œ
        
        # DPM-Solverä¸€æ­¥å»å™ªï¼šx_t -> x_{t-1}
        noisy_action = self.noise_scheduler_sample.step(
            model_output,  # æ¨¡å‹é¢„æµ‹
            t,             # å½“å‰æ—¶é—´æ­¥
            noisy_action   # å½“å‰å¸¦å™ªåŠ¨ä½œ
        ).prev_sample  # è¾“å‡º: (B, 64, 128) - å»å™ªåçš„åŠ¨ä½œ
        
        noisy_action = noisy_action.to(state_traj.dtype)
    
    # æ­¥éª¤4: åº”ç”¨åŠ¨ä½œmaskï¼Œå°†æ— æ•ˆåŠ¨ä½œç»´åº¦ç½®é›¶
    noisy_action = noisy_action * action_mask
    # (B, 64, 128)
    
    return noisy_action
```

**DPM-Solveré…ç½®**ï¼š

```python
# models/rdt_runner.py: 87-91
self.noise_scheduler_sample = DPMSolverMultistepScheduler(
    num_train_timesteps=1000,
    beta_schedule='squaredcos_cap_v2',
    prediction_type='sample',
)

# set_timestepså†…éƒ¨å®ç°ï¼ˆdiffusersåº“ï¼‰ï¼š
# å°†1000ä¸ªæ—¶é—´æ­¥å‹ç¼©åˆ°5æ­¥
# ä½¿ç”¨é«˜é˜¶æ•°å€¼æ–¹æ³•ï¼ˆå¦‚Runge-Kuttaï¼‰æ±‚è§£ODE
# å…¬å¼: dx/dt = f(x_t, t) å…¶ä¸­fç”±æ¨¡å‹é¢„æµ‹
```

**DPM-Solverä¼˜åŠ¿**ï¼š
- åªéœ€5æ­¥å³å¯å®Œæˆå»å™ªï¼ˆä¼ ç»ŸDDPMéœ€è¦1000æ­¥ï¼‰
- ä½¿ç”¨é«˜é˜¶æ•°å€¼æ±‚è§£å™¨ï¼Œç²¾åº¦æ›´é«˜
- æ¨ç†é€Ÿåº¦æå‡200å€ï¼ˆ1000æ­¥ â†’ 5æ­¥ï¼‰

---

## 5. è®­ç»ƒæµç¨‹ï¼ˆå®Œæ•´ä»£ç ï¼‰

### 5.1 è®­ç»ƒä¸»å¾ªç¯ï¼ˆå®Œæ•´å®ç°ï¼‰

**å…·ä½“å®ç°ä½ç½®**ï¼š`train/train.py: 402-481`

**å®Œæ•´ä»£ç **ï¼š

```python
# train/train.py: 402-481
for epoch in range(first_epoch, args.num_train_epochs):
    rdt.train()
    
    for batch in train_dataloader:
        # ä½¿ç”¨Accumulatorè¿›è¡Œæ¢¯åº¦ç´¯ç§¯ï¼ˆæ”¯æŒå¤šGPUå’Œæ¢¯åº¦ç´¯ç§¯ï¼‰
        with accelerator.accumulate(rdt):
            # ========== æ­¥éª¤1: å‡†å¤‡è¾“å…¥æ•°æ® ==========
            images = batch["images"].to(dtype=weight_dtype)
            # (B, 6, 3, 336, 336) - 2å†å²Ã—3ç›¸æœº
            
            states = batch["states"].to(dtype=weight_dtype)
            # (B, T, 128) - å¯èƒ½åŒ…å«å¤šä¸ªæ—¶é—´æ­¥
            
            states = states[:, -1:, :]  # åªä½¿ç”¨æœ€åä¸€ä¸ªçŠ¶æ€
            # (B, 1, 128)
            
            actions = batch["actions"].to(dtype=weight_dtype)
            # (B, 64, 128) - æœªæ¥64æ­¥åŠ¨ä½œåºåˆ—
            
            state_elem_mask = batch["state_elem_mask"].to(dtype=weight_dtype)
            # (B, 1, 128) - åŠ¨ä½œmask
            
            ctrl_freqs = batch["ctrl_freqs"]  # (B,) - æ§åˆ¶é¢‘ç‡
            
            # ========== æ­¥éª¤2: ç¼–ç å¤šæ¨¡æ€è¾“å…¥ï¼ˆå†»ç»“ç¼–ç å™¨ï¼‰ ==========
            with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœæ˜¾å­˜
                batch_size, _, C, H, W = images.shape
                
                # å›¾åƒç¼–ç ï¼šSigLIP
                image_embeds = vision_encoder(
                    images.reshape(-1, C, H, W)  # (B*6, 3, 336, 336)
                ).detach()
                image_embeds = image_embeds.reshape(
                    (batch_size, -1, vision_encoder.hidden_size)
                )  # (B, 3174, 1152)
                
                # è¯­è¨€ç¼–ç ï¼šT5
                lang_attn_mask = batch["lang_attn_mask"]  # (B, L)
                text_embeds = batch["lang_embeds"].to(dtype=weight_dtype) \
                    if args.precomp_lang_embed \
                    else text_encoder(
                        input_ids=batch["input_ids"],
                        attention_mask=lang_attn_mask
                    )["last_hidden_state"].detach()
                # (B, L, 4096)
            
            # ========== æ­¥éª¤3: æ‰©å±•çŠ¶æ€å…ƒç´ mask ==========
            state_elem_mask = state_elem_mask.unsqueeze(1)
            # (B, 1, 1, 128)
            
            # ========== æ­¥éª¤4: å‰å‘ä¼ æ’­å¹¶è®¡ç®—æŸå¤± ==========
            # rdt.forward() è°ƒç”¨ compute_loss()
            loss = rdt(
                lang_tokens=text_embeds,      # (B, L, 4096)
                lang_attn_mask=lang_attn_mask,  # (B, L)
                img_tokens=image_embeds,      # (B, 3174, 1152)
                state_tokens=states,          # (B, 1, 128)
                action_gt=actions,           # (B, 64, 128)
                action_mask=state_elem_mask,  # (B, 1, 128)
                ctrl_freqs=ctrl_freqs         # (B,)
            )  # è¾“å‡º: æ ‡é‡æŸå¤±
            
            # ========== æ­¥éª¤5: åå‘ä¼ æ’­ ==========
            accelerator.backward(loss)
            
            # ========== æ­¥éª¤6: æ¢¯åº¦è£å‰ªå’Œä¼˜åŒ–å™¨æ›´æ–° ==========
            if accelerator.sync_gradients:
                params_to_clip = rdt.parameters()
                accelerator.clip_grad_norm_(
                    params_to_clip, args.max_grad_norm  # 1.0
                )  # é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            
            optimizer.step()      # æ›´æ–°å‚æ•°
            lr_scheduler.step()   # æ›´æ–°å­¦ä¹ ç‡
            optimizer.zero_grad(set_to_none=args.set_grads_to_none)
        
        # ========== æ­¥éª¤7: æ›´æ–°EMAæ¨¡å‹ ==========
        ema_model.step(accelerator.unwrap_model(rdt))
        
        # ========== æ­¥éª¤8: è®°å½•å’Œä¿å­˜ ==========
        if accelerator.sync_gradients:
            global_step += 1
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if global_step % args.checkpointing_period == 0:
                save_path = os.path.join(args.output_dir, f"checkpoint-{global_step}")
                accelerator.save_state(save_path)
                ema_save_path = os.path.join(save_path, "ema")
                accelerator.save_model(ema_rdt, ema_save_path)
            
            # å®šæœŸé‡‡æ ·éªŒè¯
            if args.sample_period > 0 and global_step % args.sample_period == 0:
                sample_loss_for_log = log_sample_res(...)
                accelerator.log(sample_loss_for_log, step=global_step)
            
            # è®°å½•æ—¥å¿—
            logs = {"loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0]}
            accelerator.log(logs, step=global_step)
```

**å…³é”®å‚æ•°å€¼**ï¼š
- `train_batch_size`: 32ï¼ˆæ¯ä¸ªGPUï¼‰
- `gradient_accumulation_steps`: 1
- `num_train_timesteps`: 1000ï¼ˆæ‰©æ•£æ—¶é—´æ­¥ï¼‰
- `num_inference_timesteps`: 5ï¼ˆé‡‡æ ·æ—¶é—´æ­¥ï¼‰
- `horizon`: 64ï¼ˆé¢„æµ‹åŠ¨ä½œæ­¥æ•°ï¼‰
- `action_dim`: 128ï¼ˆåŠ¨ä½œç»´åº¦ï¼‰

### 5.2 è®­ç»ƒæŠ€å·§

**1. æ¢¯åº¦ç´¯ç§¯**ï¼š
- æ”¯æŒå¤šGPUè®­ç»ƒ
- ä½¿ç”¨Accelerateåº“ç®¡ç†åˆ†å¸ƒå¼è®­ç»ƒ

**2. æ··åˆç²¾åº¦è®­ç»ƒ**ï¼š
- ä½¿ç”¨bf16ï¼ˆbfloat16ï¼‰èŠ‚çœæ˜¾å­˜
- ç¼–ç å™¨ä½¿ç”¨fp32ï¼ŒRDTä½¿ç”¨bf16

**3. EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰**ï¼š
- ç»´æŠ¤å‚æ•°çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡
- ç”¨äºæ›´ç¨³å®šçš„æ¨ç†ï¼ˆå½“å‰å®ç°æœªå¯ç”¨ï¼‰

**4. å­¦ä¹ ç‡è°ƒåº¦**ï¼š
- æ”¯æŒconstantã€cosineã€linearç­‰
- é€šå¸¸ä½¿ç”¨constantå­¦ä¹ ç‡ï¼ˆ1e-4ï¼‰

**5. æ¢¯åº¦è£å‰ª**ï¼š
- é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼ˆmax_grad_norm=1.0ï¼‰

### 5.3 éªŒè¯é‡‡æ ·

**å®šæœŸé‡‡æ ·éªŒè¯**ï¼ˆæ¯500æ­¥ï¼‰ï¼š

```python
# åœ¨éªŒè¯é›†ä¸Šé‡‡æ ·ç”ŸæˆåŠ¨ä½œ
pred_actions = rdt.predict_action(...)

# è®¡ç®—MSEå’ŒL2è¯¯å·®
mse_loss = MSE(pred_actions, ground_truth)
l2_error = L2(pred_actions, ground_truth) / norm

# è®°å½•åˆ°wandb/tensorboard
log({"overall_avg_sample_mse": mse_loss})
```

**æŒ‡æ ‡è¯´æ˜**ï¼š
- `overall_avg_sample_mse`ï¼šæ•´ä½“å¹³å‡é‡‡æ ·MSEï¼ˆè¶Šä½è¶Šå¥½ï¼‰
- `overall_avg_sample_l2err`ï¼šæ•´ä½“å¹³å‡é‡‡æ ·L2è¯¯å·®ï¼ˆè¶Šä½è¶Šå¥½ï¼‰

---

## 6. æ¨ç†æµç¨‹ï¼ˆå®Œæ•´ä»£ç ï¼‰

### 6.1 æ¨ç†æ¥å£ï¼ˆå®Œæ•´å®ç°ï¼‰

**å…·ä½“å®ç°ä½ç½®**ï¼š`models/rdt_runner.py` - `predict_action`

**å®Œæ•´ä»£ç **ï¼š

```python
# models/rdt_runner.py: 318-351
def predict_action(self, lang_tokens, lang_attn_mask, img_tokens, 
                   state_tokens, action_mask, ctrl_freqs):
    """
    é¢„æµ‹åŠ¨ä½œåºåˆ—ï¼šæ¨ç†æ¥å£
    
    å‚æ•°å½¢çŠ¶ï¼š
    - lang_tokens: (B, L_lang, 4096) - T5ç¼–ç çš„è¯­è¨€token
    - lang_attn_mask: (B, L_lang) - è¯­è¨€mask
    - img_tokens: (B, 3174, 1152) - SigLIPç¼–ç çš„å›¾åƒtoken
    - state_tokens: (B, 1, 128) - å½“å‰çŠ¶æ€
    - action_mask: (B, 1, 128) - åŠ¨ä½œmask
    - ctrl_freqs: (B,) - æ§åˆ¶é¢‘ç‡
    
    è¿”å›ï¼š
    - (B, 64, 128) - é¢„æµ‹çš„åŠ¨ä½œåºåˆ—
    """
    # ========== æ­¥éª¤1: å‡†å¤‡çŠ¶æ€ ==========
    # å°†çŠ¶æ€å’Œmaskæ‹¼æ¥
    state_tokens = torch.cat([state_tokens, action_mask], dim=2)
    # (B, 1, 256) = [çŠ¶æ€(128) + mask(128)]
    
    # ========== æ­¥éª¤2: æ¡ä»¶é€‚é… ==========
    # é€šè¿‡æ¡ä»¶é€‚é…å™¨æ˜ å°„åˆ°ç»Ÿä¸€éšè—ç©ºé—´
    lang_cond, img_cond, state_traj = self.adapt_conditions(
        lang_tokens,    # (B, L_lang, 4096)
        img_tokens,     # (B, 3174, 1152)
        state_tokens    # (B, 1, 256)
    )
    # è¾“å‡ºï¼š
    # lang_cond: (B, L_lang, 2048)
    # img_cond: (B, 3174, 2048)
    # state_traj: (B, 1, 2048)
    
    # ========== æ­¥éª¤3: è¿è¡Œæ¡ä»¶é‡‡æ · ==========
    # ä»çº¯å™ªå£°ç”ŸæˆåŠ¨ä½œåºåˆ—
    action_pred = self.conditional_sample(
        lang_cond,        # (B, L_lang, 2048)
        lang_attn_mask,   # (B, L_lang)
        img_cond,         # (B, 3174, 2048)
        state_traj,       # (B, 1, 2048)
        action_mask,      # (B, 1, 128)
        ctrl_freqs,       # (B,)
    )
    # è¾“å‡º: (B, 64, 128)
    
    return action_pred
```

### 6.2 æ¨ç†æ­¥éª¤è¯¦è§£ï¼ˆå®Œæ•´å‡½æ•°è°ƒç”¨é“¾ï¼‰

**æ­¥éª¤1ï¼šå‡†å¤‡æ¡ä»¶ï¼ˆå…·ä½“ä»£ç ï¼‰**

```python
# å‡è®¾è¾“å…¥
instruction = "Pick up the cup"
images = [ext_img, right_wrist_img, left_wrist_img]  # 3å¼ å›¾åƒ
current_state = np.array([...])  # (128,) - å½“å‰æœºå™¨äººçŠ¶æ€
action_mask = np.ones(128)  # (128,) - æ‰€æœ‰ç»´åº¦æœ‰æ•ˆ

# ç¼–ç è¯­è¨€æŒ‡ä»¤
# models/multimodal_encoder/t5_encoder.py
input_ids = tokenizer(instruction, return_tensors="pt")["input_ids"]
# (1, L) L = æŒ‡ä»¤é•¿åº¦

text_embeds = text_encoder(
    input_ids=input_ids,
    attention_mask=attention_mask
)["last_hidden_state"]
# (1, L, 4096)

# ç¼–ç å¤šè§†è§’å›¾åƒ
# models/multimodal_encoder/siglip_encoder.py
image_embeds_list = []
for img in images:
    img_embed = siglip_encoder(img)  # (529, 1152)
    image_embeds_list.append(img_embed)

# å¦‚æœæœ‰å†å²å›¾åƒï¼Œéœ€è¦æ‹¼æ¥
# images = [ext_{t-1}, right_{t-1}, left_{t-1}, ext_t, right_t, left_t]
# å…±6å¼ å›¾åƒ
image_embeds = torch.cat(image_embeds_list, dim=0)
# (3174, 1152) = (6*529, 1152)

# å‡†å¤‡çŠ¶æ€
state_tokens = torch.cat([
    torch.from_numpy(current_state).unsqueeze(0),  # (1, 128)
    torch.from_numpy(action_mask).unsqueeze(0)    # (1, 128)
], dim=1)  # (1, 256)
```

**æ­¥éª¤2ï¼šæ¡ä»¶é€‚é…ï¼ˆå®Œæ•´è¿‡ç¨‹ï¼‰**

```python
# models/rdt_runner.py: 142-170
def adapt_conditions(self, lang_tokens, img_tokens, state_tokens):
    # è¯­è¨€é€‚é…å™¨
    adpated_lang = self.lang_adaptor(lang_tokens)
    # è¾“å…¥: (1, L, 4096)
    # è¿‡ç¨‹: Linear(4096â†’2048) â†’ GELU â†’ Linear(2048â†’2048)
    # è¾“å‡º: (1, L, 2048)
    
    # å›¾åƒé€‚é…å™¨
    adpated_img = self.img_adaptor(img_tokens)
    # è¾“å…¥: (1, 3174, 1152)
    # è¿‡ç¨‹: Linear(1152â†’2048) â†’ GELU â†’ Linear(2048â†’2048)
    # è¾“å‡º: (1, 3174, 2048)
    
    # çŠ¶æ€é€‚é…å™¨
    adpated_state = self.state_adaptor(state_tokens)
    # è¾“å…¥: (1, 1, 256)
    # è¿‡ç¨‹: Linear(256â†’2048) â†’ GELU â†’ Linear(2048â†’2048) â†’ GELU â†’ Linear(2048â†’2048)
    # è¾“å‡º: (1, 1, 2048)
    
    return adpated_lang, adpated_img, adpated_state
```

**æ­¥éª¤3ï¼šæ‰©æ•£é‡‡æ ·ï¼ˆå®Œæ•´è¿­ä»£è¿‡ç¨‹ï¼‰**

```python
# models/rdt_runner.py: 172-234
# åˆå§‹åŒ–çº¯å™ªå£°
noisy_action = torch.randn(1, 64, 128)  # (B, horizon, action_dim)

# DPM-Solverè®¾ç½®æ—¶é—´æ­¥
timesteps = [999, 799, 599, 399, 199]  # 5ä¸ªæ—¶é—´æ­¥

# è¿­ä»£å»å™ª
for t in timesteps:  # t = 999, 799, 599, 399, 199
    # å‡†å¤‡è¾“å…¥åºåˆ—
    action_traj = torch.cat([noisy_action, action_mask], dim=2)
    # (1, 64, 256)
    
    action_traj = self.state_adaptor(action_traj)
    # (1, 64, 2048)
    
    state_action_traj = torch.cat([state_traj, action_traj], dim=1)
    # (1, 65, 2048) = [çŠ¶æ€(1) + åŠ¨ä½œ(64)]
    
    # RDTæ¨¡å‹é¢„æµ‹
    # models/rdt/model.py: 161-226
    pred = self.model(
        state_action_traj,  # (1, 65, 2048)
        ctrl_freqs,         # (1,)
        torch.tensor([t]),  # (1,)
        lang_cond,          # (1, L, 2048)
        img_cond,           # (1, 3174, 2048)
        lang_mask=lang_attn_mask
    )
    # å†…éƒ¨è¿‡ç¨‹ï¼š
    # 1. æ—¶é—´æ­¥åµŒå…¥: t â†’ (1, 2048)
    # 2. é¢‘ç‡åµŒå…¥: freq â†’ (1, 2048)
    # 3. æ‹¼æ¥åºåˆ—: [t, freq, state, actions] â†’ (1, 67, 2048)
    # 4. æ·»åŠ ä½ç½®ç¼–ç 
    # 5. 28å±‚RDT Blockï¼ˆäº¤æ›¿æ³¨å…¥è¯­è¨€/å›¾åƒæ¡ä»¶ï¼‰
    # 6. è¾“å‡ºå±‚: (1, 67, 2048) â†’ (1, 67, 128)
    # 7. åªä¿ç•™åŠ¨ä½œ: (1, 64, 128)
    
    # DPM-Solverä¸€æ­¥å»å™ª
    noisy_action = self.noise_scheduler_sample.step(
        model_output=pred,     # (1, 64, 128)
        timestep=t,            # æ ‡é‡
        sample=noisy_action     # (1, 64, 128)
    ).prev_sample
    # è¾“å‡º: (1, 64, 128) - å»å™ªåçš„åŠ¨ä½œ
    
    # æ•°æ®å½¢çŠ¶å˜åŒ–ï¼š
    # t=999: å™ªå£°å¾ˆå¤§ â†’ t=799: å™ªå£°å‡å° â†’ ... â†’ t=199: æ¥è¿‘çœŸå®åŠ¨ä½œ

# åº”ç”¨mask
action_pred = noisy_action * action_mask
# (1, 64, 128)
```

**å®Œæ•´çš„æ¨ç†æ—¶é—´çº¿**ï¼š

```
t=0 (åˆå§‹åŒ–):
  noisy_action = N(0, I)  # çº¯å™ªå£°

t=999 (ç¬¬1æ­¥):
  pred = RDT(noisy_action, t=999, conditions)
  noisy_action = DPM_Solver_step(pred, noisy_action, t=999)
  # å™ªå£°å‡å°‘çº¦20%

t=799 (ç¬¬2æ­¥):
  pred = RDT(noisy_action, t=799, conditions)
  noisy_action = DPM_Solver_step(pred, noisy_action, t=799)
  # å™ªå£°è¿›ä¸€æ­¥å‡å°‘

... (ç»§ç»­3æ­¥)

t=199 (ç¬¬5æ­¥):
  pred = RDT(noisy_action, t=199, conditions)
  noisy_action = DPM_Solver_step(pred, noisy_action, t=199)
  # åŸºæœ¬å»å™ªå®Œæˆ

æœ€ç»ˆè¾“å‡º:
  action_pred = noisy_action * action_mask
  # (1, 64, 128) - é¢„æµ‹çš„64æ­¥åŠ¨ä½œåºåˆ—
```

### 6.3 éƒ¨ç½²ä¼˜åŒ–

**æ˜¾å­˜ä¼˜åŒ–**ï¼š
- é¢„è®¡ç®—è¯­è¨€åµŒå…¥ï¼ˆé¿å…åŠ è½½T5-XXLåˆ°GPUï¼‰
- ä½¿ç”¨é‡åŒ–ï¼ˆ4-bit/8-bitï¼‰
- ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰

**é€Ÿåº¦ä¼˜åŒ–**ï¼š
- ä½¿ç”¨Flash AttentionåŠ é€Ÿ
- ä½¿ç”¨TensorRTç­‰æ¨ç†å¼•æ“
- æ‰¹é‡æ¨ç†

---

## 7. å…³é”®å‡½æ•°è°ƒç”¨é“¾

### 7.1 è®­ç»ƒæ—¶å®Œæ•´è°ƒç”¨é“¾

```
train/train.py: train()
    â†“
train/train.py: 403 - for batch in train_dataloader
    â†“
train/train.py: 437 - loss = rdt(...)
    â†“
models/rdt_runner.py: 357 - forward()
    â†“
models/rdt_runner.py: 237 - compute_loss()
    â†“
models/rdt_runner.py: 293 - adapt_conditions()
    â†“
models/rdt_runner.py: 154-168 - lang_adaptor/img_adaptor/state_adaptor
    â†“
models/rdt_runner.py: 298 - self.model()
    â†“
models/rdt/model.py: 161 - forward()
    â†“
models/rdt/model.py: 214 - for block in self.blocks
    â†“
models/rdt/blocks.py: 213 - RDTBlock.forward()
    â†“
models/rdt/blocks.py: 228 - self.attn() [è‡ªæ³¨æ„åŠ›]
    â†“
models/rdt/blocks.py: 234 - self.cross_attn() [äº¤å‰æ³¨æ„åŠ›]
    â†“
models/rdt/blocks.py: 240 - self.ffn() [å‰é¦ˆç½‘ç»œ]
    â†“
models/rdt/model.py: 220 - self.final_layer()
    â†“
models/rdt/model.py: 225 - return x[:, -64:]  # åªä¿ç•™åŠ¨ä½œtokens
    â†“
models/rdt_runner.py: 314 - F.mse_loss(pred, target)
    â†“
è¿”å›æŸå¤±å€¼
```

### 7.2 æ¨ç†æ—¶å®Œæ•´è°ƒç”¨é“¾

```
scripts/agilex_inference.py: model.step()
    â†“
models/rdt_runner.py: 318 - predict_action()
    â†“
models/rdt_runner.py: 342 - adapt_conditions()
    â†“
models/rdt_runner.py: 346 - conditional_sample()
    â†“
models/rdt_runner.py: 210 - for t in timesteps (5æ¬¡è¿­ä»£)
    â†“
models/rdt_runner.py: 221 - self.model()
    â†“
models/rdt/model.py: 161 - forward() [åŒä¸Š]
    â†“
models/rdt_runner.py: 227 - noise_scheduler_sample.step()
    â†“
è¿”å›åŠ¨ä½œåºåˆ— (B, 64, 128)
```

### 7.3 æ•°æ®æµå½¢çŠ¶å˜åŒ–æ€»è§ˆ

**è®­ç»ƒæ—¶**ï¼š
```
æ‰¹æ¬¡æ•°æ®:
  images: (32, 6, 3, 336, 336)
  states: (32, 1, 128)
  actions: (32, 64, 128)
    â†“
ç¼–ç :
  image_embeds: (32, 3174, 1152)
  text_embeds: (32, L, 4096)
    â†“
æ¡ä»¶é€‚é…:
  img_cond: (32, 3174, 2048)
  lang_cond: (32, L, 2048)
    â†“
æ‰©æ•£åŠ å™ª:
  noisy_action: (32, 64, 128)
    â†“
çŠ¶æ€-åŠ¨ä½œåºåˆ—:
  state_action_traj: (32, 65, 2048)
    â†“
RDT Transformer:
  (32, 65, 2048) â†’ 28å±‚ â†’ (32, 65, 2048)
    â†“
è¾“å‡ºå±‚:
  (32, 65, 2048) â†’ (32, 65, 128)
    â†“
åªä¿ç•™åŠ¨ä½œ:
  pred: (32, 64, 128)
    â†“
è®¡ç®—æŸå¤±:
  loss: æ ‡é‡
```

**æ¨ç†æ—¶**ï¼š
```
è¾“å…¥:
  current_state: (1, 128)
  images: (1, 6, 3, 336, 336)
  instruction: "Pick up the cup"
    â†“
ç¼–ç :
  image_embeds: (1, 3174, 1152)
  text_embeds: (1, L, 4096)
    â†“
æ¡ä»¶é€‚é…:
  img_cond: (1, 3174, 2048)
  lang_cond: (1, L, 2048)
  state_traj: (1, 1, 2048)
    â†“
åˆå§‹åŒ–å™ªå£°:
  noisy_action: (1, 64, 128)
    â†“
DPM-Solverè¿­ä»£ (5æ­¥):
  æ¯æ¬¡: (1, 64, 128) â†’ RDT â†’ (1, 64, 128) â†’ DPM-Solver â†’ (1, 64, 128)
    â†“
æœ€ç»ˆè¾“å‡º:
  action_pred: (1, 64, 128)
```

---

## 8. å…³é”®è®¾è®¡é€‰æ‹©

### 8.1 ä¸ºä»€ä¹ˆä½¿ç”¨æ‰©æ•£æ¨¡å‹ï¼Ÿ

**ä¼˜åŠ¿**ï¼š
1. **å¤šæ¨¡æ€ç”Ÿæˆ**ï¼šè‡ªç„¶åœ°å¤„ç†å¤šæ¨¡æ€è¾“å‡ºï¼ˆ64æ­¥åŠ¨ä½œåºåˆ—ï¼‰
2. **ç¨³å®šè®­ç»ƒ**ï¼šç›¸æ¯”GANï¼Œè®­ç»ƒæ›´ç¨³å®š
3. **å¤šæ ·æ€§**ï¼šå¯ä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„åŠ¨ä½œåºåˆ—
4. **å¯è§£é‡Šæ€§**ï¼šå»å™ªè¿‡ç¨‹å¯ä»¥å¯è§†åŒ–

### 8.2 ä¸ºä»€ä¹ˆä½¿ç”¨Transformerï¼Ÿ

**ä¼˜åŠ¿**ï¼š
1. **åºåˆ—å»ºæ¨¡**ï¼šè‡ªç„¶åœ°å¤„ç†åŠ¨ä½œåºåˆ—
2. **å¤šæ¨¡æ€èåˆ**ï¼šé€šè¿‡äº¤å‰æ³¨æ„åŠ›èåˆå¤šæ¨¡æ€æ¡ä»¶
3. **å¯æ‰©å±•æ€§**ï¼šæ˜“äºæ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹
4. **å¹¶è¡Œè®­ç»ƒ**ï¼šæ”¯æŒé«˜æ•ˆçš„å¹¶è¡Œè®­ç»ƒ

### 8.3 ä¸ºä»€ä¹ˆä½¿ç”¨æ¡ä»¶æ©ç è®­ç»ƒï¼Ÿ

**ç›®çš„**ï¼š
1. **é²æ£’æ€§**ï¼šè®©æ¨¡å‹åœ¨æ¡ä»¶ç¼ºå¤±æ—¶ä¹Ÿèƒ½å·¥ä½œ
2. **æ³›åŒ–æ€§**ï¼šæé«˜å¯¹ä¸åŒåœºæ™¯çš„æ³›åŒ–èƒ½åŠ›
3. **é›¶æ ·æœ¬èƒ½åŠ›**ï¼šæ”¯æŒé›¶æ ·æœ¬è¿ç§»åˆ°æ–°ä»»åŠ¡

---

## 9. æ€»ç»“

### 9.1 ç®—æ³•æµç¨‹æ€»è§ˆ

```
æ•°æ®åŠ è½½ â†’ å¤šæ¨¡æ€ç¼–ç  â†’ æ¡ä»¶é€‚é… â†’ RDT Transformer â†’ æ‰©æ•£å»å™ª â†’ åŠ¨ä½œé¢„æµ‹
```

### 9.2 æ ¸å¿ƒåˆ›æ–°ç‚¹

1. **ç»Ÿä¸€åŠ¨ä½œç©ºé—´**ï¼š128ç»´å‘é‡æ”¯æŒå¤šç§æœºå™¨äººé…ç½®
2. **å¤šæ¨¡æ€æ¡ä»¶æ³¨å…¥**ï¼šäº¤æ›¿æ³¨å…¥è¯­è¨€å’Œå›¾åƒæ¡ä»¶
3. **æ¡ä»¶æ©ç è®­ç»ƒ**ï¼šæå‡é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›
4. **é«˜æ•ˆé‡‡æ ·**ï¼šDPM-Solverä»…éœ€5æ­¥å®Œæˆæ¨ç†

### 9.3 åº”ç”¨åœºæ™¯

- **åŒè‡‚æ“ä½œ**ï¼šALOHAç­‰åŒè‡‚æœºå™¨äºº
- **ç§»åŠ¨æ“ä½œ**ï¼šMobile ALOHAç­‰ç§»åŠ¨æœºå™¨äºº
- **å•è‡‚æ“ä½œ**ï¼šé€šè¿‡é€‚é…å™¨é€‚é…å•è‡‚æœºå™¨äºº
- **ä»¿çœŸç¯å¢ƒ**ï¼šManiSkillç­‰ä»¿çœŸç¯å¢ƒ

---

## 10. å…³é”®å®ç°ç»†èŠ‚æ€»ç»“

### 10.1 æ•°æ®å½¢çŠ¶å˜åŒ–å®Œæ•´è·¯å¾„

**è®­ç»ƒæ—¶å®Œæ•´æ•°æ®æµ**ï¼š
```
åŸå§‹æ•°æ®:
  images: (32, 6, 3, 336, 336)      # æ‰¹æ¬¡Ã—å†å²Ã—ç›¸æœºÃ—é€šé“Ã—å°ºå¯¸
  states: (32, 1, 128)               # æ‰¹æ¬¡Ã—æ—¶é—´æ­¥Ã—çŠ¶æ€ç»´åº¦
  actions: (32, 64, 128)             # æ‰¹æ¬¡Ã—åŠ¨ä½œæ­¥æ•°Ã—åŠ¨ä½œç»´åº¦
    â†“
å¤šæ¨¡æ€ç¼–ç :
  image_embeds: (32, 3174, 1152)     # æ‰¹æ¬¡Ã—å›¾åƒtokensÃ—SigLIPç»´åº¦
  text_embeds: (32, L, 4096)         # æ‰¹æ¬¡Ã—è¯­è¨€tokensÃ—T5ç»´åº¦
    â†“
æ¡ä»¶é€‚é…:
  img_cond: (32, 3174, 2048)         # æ‰¹æ¬¡Ã—å›¾åƒtokensÃ—éšè—ç»´åº¦
  lang_cond: (32, L, 2048)           # æ‰¹æ¬¡Ã—è¯­è¨€tokensÃ—éšè—ç»´åº¦
    â†“
æ‰©æ•£åŠ å™ª:
  noisy_action: (32, 64, 128)        # æ‰¹æ¬¡Ã—åŠ¨ä½œæ­¥æ•°Ã—åŠ¨ä½œç»´åº¦
    â†“
çŠ¶æ€-åŠ¨ä½œåºåˆ—æ„å»º:
  state_action_traj: (32, 65, 256)   # æ‰¹æ¬¡Ã—[çŠ¶æ€+åŠ¨ä½œ]Ã—[å€¼+mask]
    â†“
çŠ¶æ€é€‚é…å™¨:
  state_action_traj: (32, 65, 2048)  # æ‰¹æ¬¡Ã—åºåˆ—é•¿åº¦Ã—éšè—ç»´åº¦
    â†“
RDT Transformerè¾“å…¥:
  [æ—¶é—´æ­¥, é¢‘ç‡, çŠ¶æ€, åŠ¨ä½œ]: (32, 67, 2048)  # 1+1+1+64=67
    â†“
28å±‚RDT Block:
  (32, 67, 2048) â†’ ... â†’ (32, 67, 2048)
    â†“
è¾“å‡ºå±‚:
  (32, 67, 2048) â†’ (32, 67, 128)
    â†“
åªä¿ç•™åŠ¨ä½œ:
  pred: (32, 64, 128)
    â†“
è®¡ç®—æŸå¤±:
  loss: æ ‡é‡
```

**æ¨ç†æ—¶å®Œæ•´æ•°æ®æµ**ï¼š
```
è¾“å…¥:
  current_state: (1, 128)
  images: (1, 6, 3, 336, 336)
  instruction: "Pick up the cup"
    â†“
ç¼–ç :
  image_embeds: (1, 3174, 1152)
  text_embeds: (1, L, 4096)
    â†“
æ¡ä»¶é€‚é…:
  img_cond: (1, 3174, 2048)
  lang_cond: (1, L, 2048)
  state_traj: (1, 1, 2048)
    â†“
åˆå§‹åŒ–å™ªå£°:
  noisy_action: (1, 64, 128)
    â†“
DPM-Solverè¿­ä»£ (5æ­¥):
  æ¯æ¬¡è¿­ä»£:
    action_traj: (1, 64, 256) â†’ state_adaptor â†’ (1, 64, 2048)
    state_action_traj: (1, 65, 2048)
    RDTé¢„æµ‹: (1, 65, 2048) â†’ (1, 64, 128)
    DPM-Solverå»å™ª: (1, 64, 128) â†’ (1, 64, 128)
    â†“
æœ€ç»ˆè¾“å‡º:
  action_pred: (1, 64, 128)
```

### 10.2 å…³é”®å‚æ•°é…ç½®è¡¨

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `hidden_size` | 2048 | RDTéšè—å±‚ç»´åº¦ |
| `depth` | 28 | RDT Blockå±‚æ•° |
| `num_heads` | 32 | å¤šå¤´æ³¨æ„åŠ›å¤´æ•° |
| `head_dim` | 64 | æ¯å¤´ç»´åº¦ (2048/32) |
| `horizon` | 64 | é¢„æµ‹åŠ¨ä½œæ­¥æ•° |
| `action_dim` | 128 | åŠ¨ä½œå‘é‡ç»´åº¦ |
| `num_train_timesteps` | 1000 | æ‰©æ•£è®­ç»ƒæ—¶é—´æ­¥ |
| `num_inference_timesteps` | 5 | DPM-Solveré‡‡æ ·æ­¥æ•° |
| `img_history_size` | 2 | å†å²å›¾åƒæ•°é‡ |
| `num_cameras` | 3 | ç›¸æœºæ•°é‡ |
| `max_lang_cond_len` | 1024 | æœ€å¤§è¯­è¨€tokenæ•° |
| `img_cond_len` | 3174 | å›¾åƒtokenæ•° (6Ã—529) |
| `lang_token_dim` | 4096 | T5ç¼–ç ç»´åº¦ |
| `img_token_dim` | 1152 | SigLIPç¼–ç ç»´åº¦ |
| `state_token_dim` | 128 | çŠ¶æ€å‘é‡ç»´åº¦ |

### 10.3 å…³é”®æ•°å­¦å…¬å¼

**1. æ‰©æ•£å‰å‘è¿‡ç¨‹ï¼ˆè®­ç»ƒï¼‰**ï¼š
```
x_t = âˆš(Î±_t) Â· x_0 + âˆš(1 - Î±_t) Â· Îµ
å…¶ä¸­:
  x_0: çœŸå®åŠ¨ä½œ (B, 64, 128)
  Îµ: å™ªå£° ~ N(0, I)
  Î±_t: ç´¯ç§¯ä¹˜ç§¯ï¼ŒÎ±_t = âˆ_{s=1}^t (1 - Î²_s)
  Î²_s: å™ªå£°è°ƒåº¦ï¼ˆsquaredcos_cap_v2ï¼‰
```

**2. æ‰©æ•£åå‘è¿‡ç¨‹ï¼ˆæ¨ç†ï¼‰**ï¼š
```
x_{t-1} = DPM_Solver_step(x_t, f(x_t, t), t)
å…¶ä¸­:
  f(x_t, t) = model(x_t, t, conditions)  # æ¨¡å‹é¢„æµ‹
  DPM-Solverä½¿ç”¨é«˜é˜¶æ•°å€¼æ–¹æ³•æ±‚è§£ODE
```

**3. äº¤å‰æ³¨æ„åŠ›**ï¼š
```
Q = Linear_q(x)  # (B, N, 2048) â†’ (B, N, 2048)
K, V = Linear_kv(c)  # (B, L, 2048) â†’ (B, L, 2048Ã—2)
Attention = softmax(QK^T / âˆšd_k) V
è¾“å‡º = Attention + xï¼ˆæ®‹å·®ï¼‰
```

**4. æŸå¤±å‡½æ•°**ï¼š
```
L = MSE(pred, target)
å…¶ä¸­:
  pred = RDT(noisy_action, t, conditions)  # (B, 64, 128)
  target = action_gt  # (B, 64, 128) å¦‚æœprediction_type='sample'
```

### 10.4 æ€§èƒ½ä¼˜åŒ–æŠ€å·§

**1. æ˜¾å­˜ä¼˜åŒ–**ï¼š
- å†»ç»“ç¼–ç å™¨ï¼ˆT5å’ŒSigLIPä¸è®¡ç®—æ¢¯åº¦ï¼‰
- ä½¿ç”¨bf16æ··åˆç²¾åº¦
- é¢„è®¡ç®—è¯­è¨€åµŒå…¥ï¼ˆé¿å…åŠ è½½T5-XXLåˆ°GPUï¼‰
- DeepSpeed ZeRO-2ä¼˜åŒ–

**2. é€Ÿåº¦ä¼˜åŒ–**ï¼š
- Flash AttentionåŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—
- DPM-Solveråªéœ€5æ­¥æ¨ç†ï¼ˆvs 1000æ­¥DDPMï¼‰
- æ‰¹é‡ç¼–ç å›¾åƒï¼ˆä¸€æ¬¡æ€§ç¼–ç æ‰€æœ‰å›¾åƒï¼‰

**3. è®­ç»ƒç¨³å®šæ€§**ï¼š
- æ¢¯åº¦è£å‰ªï¼ˆmax_grad_norm=1.0ï¼‰
- RMSå½’ä¸€åŒ–ï¼ˆæ¯”LayerNormæ›´ç¨³å®šï¼‰
- é›¶åˆå§‹åŒ–æœ€ç»ˆå±‚ï¼ˆé¿å…è®­ç»ƒåˆæœŸä¸ç¨³å®šï¼‰

---

## å‚è€ƒèµ„æ–™

- è®ºæ–‡ï¼šRDT-1B: a Diffusion Foundation Model for Bimanual Manipulation
- ä»£ç ä»“åº“ï¼šhttps://github.com/thu-ml/RoboticsDiffusionTransformer
- æ¨¡å‹æƒé‡ï¼šhttps://huggingface.co/robotics-diffusion-transformer/rdt-1b

